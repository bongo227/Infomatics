{
    "docs": [
        {
            "location": "/",
            "text": ".latex-box.math-false {text-align: center;}\n.math-true {vertical-align: middle;}\n\n\n\nInfomatics Notes",
            "title": "Home"
        },
        {
            "location": "/#infomatics-notes",
            "text": "",
            "title": "Infomatics Notes"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/",
            "text": ".latex-box.math-false {text-align: center;}\n.math-true {vertical-align: middle;}\n\n\n\nPropositonal Logic\n\n\nPropositions\n\n\n\n\nProposition\n\n\nA declarative sentence (declares a fact) that is either true or false.\n\n\n\n\n\n\n\n\n\n\nPropositon\n\n\nNot propositions\n\n\n\n\n\n\n\n\n\n\nIt is raining.\n\n\nWhat is the data?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogical operators\n\n\nFrom these propositions we can from new compound propositions with logical operators:\n\n\n\n\n\n\n\n\nOperation\n\n\nExample\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nNegation\n\n\n\n\nit is not the case that \n\n\n\n\n\n\nConjunction\n\n\n\n\nboth \n and \n must be true for \n to be true\n\n\n\n\n\n\nDisjunction\n\n\n\n\nFalse when both \n and \n are false, true otherwise\n\n\n\n\n\n\nExclusive Or\n\n\n\n\nEither \n or \n must be true (\nnot both\n) for \n to be true\n\n\n\n\n\n\nImplication\n\n\n\n\nFalse when \n is true and \n is false, true otherwise. \n is the hypothesis/antecedent and \n is the conclusion/conseqence\n\n\n\n\n\n\nBiconditional\n\n\n\n\n if and only if \n, true when both \n and \n have the same truth values\n\n\n\n\n\n\n\n\nTruth tables\n\n\nWith these logical operators we can build complex logical statements such as: \n. To construct truth tables for these expressions we start with the individual variables \n and \n, and solve sub expressions until we reach the full expression.\n\n\n\n\nOperator precedence\n\n\nThe order of which operators apply are as follows:\n\n\n\n\n\n\n\n\nOperator\n\n\nPrecedence\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\nLogical Equivalences\n\n\n\n\nTautology\n\n\nA compound proposition that is \ntrue\n, no matter what values the propositional variables hold.\n\n\nContradiction\n\n\nA compound proposition that is \nfalse\n, no matter what values the propositional variables hold.\n\n\nContingency\n\n\nCompound propositions that are neither a tautology or a contradiction.\n\n\n\n\n\n\n is a tautology\n\n\n is a contradiction\n\n\n is a contingency\n\n\n\n\nTwo compound propositions are \nlogically equivilent\n if they have the same truth values in all cases. This occors when \n is a tautology, where \n and \n are compound propositions. \n is a statement, denoting that \n and \n are logically equivilent.\n\n\nImportant Equivalences\n\n\n\n\n\n\n\n\nEquivalence\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\nIdentity laws\n\n\n\n\n\n\n\n\nDomination laws\n\n\n\n\n\n\n\n\nIdempotent laws\n\n\n\n\n\n\n\n\nDouble negation law\n\n\n\n\n\n\n\n\nCommutative laws\n\n\n\n\n\n\n\n\nAssociative laws\n\n\n\n\n\n\n\n\nDistributive laws\n\n\n\n\n\n\n\n\nDe Morgan's laws\n\n\n\n\n\n\n\n\nAbsorption laws\n\n\n\n\n\n\n\n\nNegation laws\n\n\n\n\n\n\n\n\nLogical equivalences involving conditional statements\n\n\n\n\n\n\n\n\nLogical equivalences involving biconditionals\n\n\n\n\n\n\n\n\nNotation\n\n\nSimular to \n, their exsists a notation for \n and \n.\n\n\n\n\n\n\nConstructing new logical equivalences\n\n\nUsing equivalences we already know, we can prove expressions are tautologys or that two expressions are logically equivelent. While a truth table could also be used, it is much shorted to apply laws.\n\n\n\n\nShow that \n is a tautology.\n\n\n\n\n\nSatisfaction\n\n\n\n\nSatisfiable\n\n\nA compound proposition were there \nis a\n assignment of truth values that make it true.\n\n\nUnsatisfiable\n\n\nA compound proposition were there \nis no\n assignment of truth values that make it true.\n\n\n\n\nThe values which make a compound proposition true is called the \nsolution to the satisfiability problem\n.\n\n\nPredicates and Quantifiers\n\n\nPredicates\n\n\nStatements that involve variables such as \n and \n are undecided when their variables are not given a value. We need a more powerfull logic, \npredicate logic\n, to model these statements.\n\n\nWe turn the statement \n into the predicate \n where \n is \n. When we give \n a value, the statement becomes a proposition, e.g. \n becomes \n which is \n.\n\n\nA predicate can have any number of variables, \n. A predicate with \n variables is called an \n-ary predicate.\n\n\nQuantifiers\n\n\nAssigning variables is not the only way to form a proposition from a predicate. \nQuantification\n expresses the extend to which a predicate is true over a range.\n\n\n\n\n\n\n\n\nName\n\n\nExample\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nUniversal quantifier\n\n\n\n\nis true if \n is true for all x (domain)\n\n\n\n\n\n\nExistential quantifier\n\n\n\n\nis true if \n is true for at least one value in the domain\n\n\n\n\n\n\n\n\n\n\nIs \n where the domain is all real numbers true.\n\n\nNo, if \n, \n) is false, thus not all elements in the domain make \n true.\n\n\n\n\nRestricting the domain\n\n\nConsider \n where the domain of \n is \n. We could express that as:\n\n\n\n\nTheir is a shorter notation for restricting the domain\n\n\n\n\nPrecedence of Quantifiers\n\n\n and \n have a higher precedence that all other logical operators.\n\n\nBinding Variables\n\n\nWhen a quantifier is used on variable, or the variable is assigned a value, the variable is said to be \nbound\n. Predicates must have all the variables bound to be turned into a proposition. Variables that are not bound are \nfree\n.\n\n\nThe \nscope\n of a quantifier is the part of the logical expression to which it is applied, hence variables outside of the scope of all quantifiers are free (if their value has not been assigned).\n\n\nLogical Equivalence\n\n\nTwo expressions involving predicates and quantifiers are equal if their truth values are the same throughout the domain.\n\n\n\n\nShow that \n and \n are logically equivalent.\n\n\n\n\nLet \n be some element in the domain, thus \n must be \n.\n\n\nIf \n is \n, \n and \n must both be \n.\n\n\nSince \n and \n are \n for all element in the domain, \n must also be $\\top$.\n\n\n\n\nSo\n\n\n\n\n\nNegation\n\n\n\n\n\n\n\n\nStatement\n\n\nEquivalent\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTheir is an \n for which \n is \n\n\n\n\n\n\n\n\n\n\nFor every \n, \n is \n\n\n\n\n\n\n\n\n\n\nProve \n\n\n\n\n is \n, if and only if \n is \n\n\nIf \n is \n, then their is atleast one element in the domain for which \n is true.\n\n\nIt follows then the \n\n\n\n\n\n\nNested Quantifiers\n\n\nMore complex statements often involve more than one quantifier. For example the logical statement that \nif \n is positive and \n is negative, \n is negative\n can be expressed as:\n\n\n\n\nProof\n\n\nRules of Inference\n\n\nProofs are mathmatical arguments that determine the truth of a statement. \nBy an argument\n, means a sequence of statements that end in a conclusion. \nBy valid\n, means the conclusion must follow from the arguments. \nBy premise\n, means the argument is valid if and only if it is impossible for all premises to be true and the conclusion false.\n\n\nAn \nargument\n is propositional logic is a sequence of propositons, where the final proposition is the conclusion and all others are the premises. An \nargument form\n in propositional logic is a sequence of compound propositions that is \nvalid\n no matter what propositions are substitude for a premise, so long as the premises are true.\n\n\nThe \nmodus ponens\n (law of detachment) is one rule we can use to proof statements. \n\n\n\n\n\nLet \n be \"it snows\" and \n be \"will go skiing\", if \"we will go skiing if it snows\" is \"we will go skiing\" true?\n\n\n\n\n\n\n\n\n\n\n\n\nRule of Inference\n\n\nTautology\n\n\nName\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModus ponens\n\n\n\n\n\n\n\n\n\n\nModus tollens\n\n\n\n\n\n\n\n\n\n\nHypothetical syllogism\n\n\n\n\n\n\n\n\n\n\nDisjunctive syllogism\n\n\n\n\n\n\n\n\n\n\nAddition\n\n\n\n\n\n\n\n\n\n\nSimplification\n\n\n\n\n\n\n\n\n\n\nConjunction\n\n\n\n\n\n\n\n\n\n\nResolution\n\n\n\n\n\n\n\n\nUsing Rules of Inference to Build Arguments\n\n\n\n\nGiven the premesis: \n, \n, \n, \n find an argument that shows the premisies lead to the conclusion \n.\n\n\n\n\n\n\nFallacies\n\n\n is a tautology, fallacies often look like tautologys but are just contingencies:\n\n\n\n\nFallacy of affirming the conclusion\n\n\noccors when \n is treated as a tautology, but it is false when \n is false and \n is true.\n\n\nFallacy of denying the hypothesis\n\n\noccors when \n is treated as a tautology, but it is false when \n is false and \n is true.\n\n\n\n\nRules of Inference for Quantified Statements\n\n\n\n\n\n\n\n\nRule of Inference\n\n\nName\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n\nUniversal instantiation\n\n\n is true, where \n is an element of the domain, since all elements are true.\n\n\n\n\n\n\n\n\nUniversal generalization\n\n\n is true, if \n (where \n is an arbitary element in the domain) is true. Since we do not make any assumptions about \n, other than its an element of the domain, all values of the domain must be true.\n\n\n\n\n\n\n\n\nExistential instantiation\n\n\nTheir is an element \n in the domain that makes \n true. Note \n is not arbitary since some elements in the domain do not make \n true.\n\n\n\n\n\n\n\n\nExistential generalization\n\n\nIf we know one element \n in the domain that makes \n true, then it follows that \n\n\n\n\n\n\n\n\n\n\nShow that the premesis \"A student in the class has not read the book\" and \"Everyone in the class passed the exam\" implys the conclusion that \"Someone who passed the exam had not read the book\"\n\n\n\n\nLet \n be \"\n is in the class\"\n\n\nLet \n be \"\n passed the exam\"\n\n\nLet \n be \"\n has read the book\"\n\n\n\n\n\n\n\n\nSets, Functions, Relations, Sequences and Sums\n\n\nSet\n\n\n\n\nSet\n\n\nAn unordered collection of elements\n\n\n\n\nSet Operations\n\n\n\n\n\n\n\n\nName\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nEquality\n\n\n\n\n\n\n\n\nMembership\n\n\n\n\n\n\n\n\nNon-Membership\n\n\n\n\n\n\n\n\nEmpty Set\n\n\n\n\n\n\n\n\nUnion\n\n\n\n\n\n\n\n\nIntersection\n\n\n\n\n\n\n\n\nDiffrence\n\n\n\n\n\n\n\n\nComplement\n\n\n\n\n\n\n\n\nSubset\n\n\n\n\n\n\n\n\nSuperset\n\n\n\n\n\n\n\n\nPower Set\n\n\n (set of all subsets)\n\n\n\n\n\n\nCardinality\n\n\n\n\n\n\n\n\nCartesian Product\n\n\n\n\n\n\n\n\n\n\nCommon Sets\n\n\n\n\n\n\n\n\nName\n\n\nSymbol\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nBooleans\n\n\n\n\n\n\n\n\n\n\nNatural numbers\n\n\n\n\n\n\n\n\n\n\nInteger\n\n\n\n\n\n\n\n\n\n\nPositive Integers\n\n\n\n\n\n\n\n\n\n\nReal Numbers\n\n\n\n\n\n\n\n\n\n\nRational\n\n\n\n\n\n\n\n\n\n\nComplex Numbers\n\n\n\n\n\n\n\n\n\n\n\n\nSet Identitys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions\n\n\nSyntax\n\n\n\n\nFunction\n\n\nA function \n from set \n to set \n assigns an element from \n to \n, this is written as \n.\n\n\n\n\nExamples:\n\n\n\n\nCeil - \n\n\nFloor - \n\n\nIdentity - \n\n\nFactorial - \n\n\n\n\n\n\n\n\n\n\nType of function\n\n\nCondition\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nInjective\n\n\n\n\n, \n\n\n\n\n\n\nSurjective\n\n\n\n\n, \n\n\n\n\n\n\nBijection\n\n\nInjective and surjective\n\n\n, \n\n\n\n\n\n\n\n\nComposition\n\n\nLet \n and \n. The composition function \n is \n\n\n\n\nThe composition of two functions is a function\n\n\nThe composition of two injective functions is a injective function\n\n\nThe composition of two surjective functions is a surjective function\n\n\nThe composition of two bijections is a bijection\n\n\n\n\nInverse\n\n\nIf \n is a bijection, then the inverse of \n, written \n is \n iff \n\n\n\n\nInverse of the identity function, is the identity function\n\n\nInverse of \n is \n \n\n\n and \n is the identity function\n\n\n\n\nRelations\n\n\nA binary relation \n on sets \n and \n is a subset \n\n\n\n\nOften written as \n for \n\n\nA function \n is a restricted relation where \n\n\nFor all a in \n their exsists a \n in \n\n\nwhere (a, b) in relation \n\n\nand \n is unique  \n\n\n\n\n\n\nGiven sets \n a subset \n is an \n-ary relation.\n\n\n\n\n where \n is students and \n is courses, \n\n\nRelation composition\n\n\nLet \n and \n. The composition relation \n is \n\n\nThe expression specifys the relations where \n and \n have a common \n, thus \n\n\nClosure \n is a relation on \n:\n\n\n\n\n is the identity relation \n\n\n\n\n\n\n\n\nTypes of relation\n\n\n\n\n\n\n\n\nType of relation\n\n\nCondition\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nReflexive\n\n\n\n\n, \n\n\n\n\n\n\nSymetric\n\n\n\n\n\n\n\n\n\n\nAntisymetric\n\n\n\n\n, \n, \n\n\n\n\n\n\nTransitive\n\n\n\n\n, \n, \n\n\n\n\n\n\nEquivalance\n\n\nReflexive, symetric and transitive\n\n\n\n\n\n\n\n\n\n\nEquivalance classes\n\n\nLet \n be an equivalence realtion on a set \n and \n let \n be the equivalance class of a w.r.t \n.\n\n\nIf \n the \n is called a representative of the equivalance class.\n\n\nLet \n be an equivalance realtion on \n amd \n. Then following statements are equivalnet\n\n\n\n\n\n\n\n\n\n\n\n\nPartitions of a Set\n\n\nA partition of set \n is a collection of disjoint, nonempty subsets that have \n as their union, in other words the collection of subsets \n with \n (where \n is the index set) forms a partition of \n iff\n\n\n\n\n for all \n\n\n for all \n\n\n\n\n\n\nFrom this can see:\n\n\n\n\nIf \n is an equivalance class on \n, then the equivalance classes of \n form a partion of \n.\n\n\nConversly, given a partition \n of \n, there exsists an equivalance relation \n that has the sets \n, \n, as its equivalence classes.\n\n\n\n\nSequences\n\n\n\n\nSequence\n\n\nOrdered list of elements for example, \n is \n defines the sequence \n\n\nGeometric progressions\n\n\nA sequence in the form \n\n\nArithmetic progressions\n\n\nA sequence in the form \n\n\n\n\nRecurrence relations\n\n\n\n\nRecurrence relations\n\n\nA recurance relations for \n is an equation that expreses \n in terms of one or more elements of \n\n\n\n\n\n\nAfter 1 month a pair of rabbits is placed on an island\n\n\nAfter every 2 months, each pair of rabbits produces a new pait of rabbits \n\n\n\n\nThe realtion is:\n\nERROR\n\n\nSolving a recurrence relations if finding the nth term, one way of solving is an iterative aproach\n\n\n\n\nSupose a person deposits $1000 in savings with 3 percent intrest, how much is the account worth after 20 years\n\n\n\n\nLet \n denote the amount after \n years\n\n\n\n\n\n\n\n\n\n\n\n\nSumations\n\n\nGiven a sequence \n, the sum of the terms is \n\n\n\nUsefull sumations\n\n\n\n\n\n\n\n\nSum\n\n\nClosed form\n\n\n\n\n\n\n\n\n\n\n\n\n, \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\n\n\nGiven a sequence \n, the product of the terms is \n\n\n\nor more generally for a finite index set \n, \n\n\n\nCartinality\n\n\n\n\nFinite set\n\n\nA set \n is finite with cardinality \n if there is a bijection from the set \n to \n.\n\n\nInfinite set\n\n\nA set that is not finite\n\n\n\n\n\n\nProve the set of natural numbers, \n, is an infinite set.\n\n\n\n\nConsider the function \n defined as \n\n\n is injective, since every element in the domain has an element in the codomain which is mapped to it.\n\n\n is not subjective, since \n is in the codomain but is has no element in the domain which maps to it.\n\n\nThus \n is not a bijection.\n\n\nThus \n is an infinite set.\n\n\n\n\n\n\nCountable sets\n\n\n\n\nCountable\n\n\nA set that is either finite or has the same cardinality as the set of the positive integers\n\n\nUncountable\n\n\nA set that is not countable\n\n\n\n\nAn infinite set \n is countable, we denote the cardinality of \n by \n \n\n\n\n\nIf \n and \n are countable sets, then \n is also countable\n\n\nIf \n is countable and for each \n the set \n is countable then \n is countable\n\n\n\n\n\n\nShow that the set of real numbers is uncountable \n\n\n\n\nAssume that the set of real numbers is countable\n\n\nThe the real numbers between \n and \n must also be countable\n\n\nWe can list these numbers with decimal noation \n\n\nWhere \n\n\nWe can define a number \n\n\nWhere \nERROR\n\n\nSo \n defines a number that is diffrent from \n in the \n number\n\n\nThus their is a real number between \n and \n that is not in the list\n\n\nTherefore the set of real numbers cannot be listed.\n\n\n\n\n\n\nSchr\u00f6der-Bernstein Theorem\n\n\nIf \n and \n are sets with \n and \n, then \n. In other words, if there are one-to-one functions \n from \n to \n and \n from \n to \n, then there is a one-to-one correspondence between \n and \n.\n\n\n\n\nShow that \n\n\n\n\nSince \n, their is a one-to-one function (e.g. identity function) from \n to \n\n\nLets define \n, which is a mapping from \n to \n, \n so their is a one \n to \n.\n\n\nThus \n, due to Schr\u00f6der-Bernstein Theorem\n\n\n\n\n\n\nCantor\u2019s theorem\n\n\n\n\n\n\nProof\n\n\n\n\nConsider the injection \n with \n for any \n.\n\n\nTherefore \n\n\nAssume a surjection \n exsists\n\n\nLet \n\n\nSince \n is a surjection, there must exist an \n such that \n   \n\n\nIf \n then by definition of \n, \n, thus \n (contradiction)\n\n\nIf \n then \n, by definition of \n, \n (contradiction)\n\n\n\n\n\n\n\n\n\n\nOne consequence of cantors theorem is that their is an infinite hierarchy of sets of larger cardinality.\n\n\nInduction\n\n\n\n\nMathematical Induction\n\n\nTo prove \n is true for all positive integers, where \n is a propositional function\n\n\nBasic Step\n\n\nVerifying \n is true\n\n\nInductive step\n\n\nShowing that the conditional \n is true for all positive integers \n\n\n\n\n\n\nProve that if \n is a positive integer \n\n\n\n\n, thus \n is true\n\n\nAssuming \n holds, then it must be shown that \n holds \n\n\nThus if we can show \n, then \n\n\n\n\n\n\n\n\nStrong induction\n\n\n\n\nMathematical Strong Induction\n\n\nTo prove \n is true for all positive integers, where \n is a propositional function\n\n\nBasic Step\n\n\nVerifying \n is true\n\n\nInductive step\n\n\nShowing that the conditional \n is true for all positive integers \n\n\n\n\n\n\nProve that if \n, then \n can be written as the product of primes\n\n\n\n\n is true since \n (\n is a prime itself).\n\n\n is either a prime or a composite number\n\n\nIf it is prime, then itself is a product of primes\n\n\nIf it is composite, then \n is the product of two positive integers \n and \n that are \n.\n\n\nThus since both are less than \n, by strong induction they can be rewritten as the product of primes\n\n\nThus \n can be written as the product of primes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplicative Inverses\n\n\n\n\nLinear congruence\n\n\nA congruence in the form \n\n\nInverse\n\n\nAn interger \n, for which \n\n\n\n\nTheir is an efficent algorithum for computing the inverse of \n when \n. We can find the linear combination \n, reducing both sides by modulo \n gives \n as an inverse.\n\n\nExample\n\n\nFind an inverse of \n modulo \n\n\n\n\n\n\nThus \n is the inverse of \n\n\nThe chinese remainder theorem\n\n\nLet \n be pairwise relative primes greater than 1.\n\n\n\n\nThen the system above has a unique solution modulo \n\n\nExample\n\n\n\n\nFermat's little theorem\n\n\nIf \n is prime and \n is an integer not divisible by \n then:\n\n\n\n\nExample\n\n\nFind \n.\n\n\n and \n, so by fermat's little theorem \n. \n for every positive integer \n. Therefore \n. \n. Hence \n.\n\n\nCounting\n\n\nProduct rule\n\n\nIf \n and \n are finite sets \n where \n\n\nExample\n\n\nHow many diffrent car license plates can be made of 3 uppercase letters and 3 digits.\n\n\n\n\nCounting subsets\n\n\nA finite set \n has \n distinct subsets.\n\n\nProof\n\n\nTheis is bijection between the bit strings of length \n, since an arbitary bit string tells us what elements from set \n is in the subset. Thus by the product rule their are \n distinct subsets.\n\n\nCounting functions\n\n\nFor all finite sets \n and \n, the number of distinct functuons \n is \n\n\nProof\n\n\nTheir is a bijection between between functions \n and strings of length \n over an alphabet \n. This is because each string gives all the values of a function \n.\n\n\nSum rule\n\n\nIf \n and \n are disjoint then \n.\n\n\nExample\n\n\nSuppose variables can be 1 uppercase letter or 1 uppercase letter and a number\n\n\n, since the sets are disjoint.\n\n\nSubtraction rule\n\n\nFor sets \n and \n, \n.\n\n\nExample\n\n\nHow many bit strings of lenghth 8 either start with a 1 bit, or end with two 00 bits.\n\n\n\n\nThe pigeonhole principle\n\n\nFor any positive integer \n, if \n objects, are placed in \n boxes,then at least one box contains two or more objects.\n\n\nOr more formally, if \n maps finite set \n with \n\n\nGeneralized pigeon hole\n\n\nIf \n objects placed in \n boxes, then at least one box contains at least \n.\n\n\nExample\n\n\nAtleast \n students were born in the dame month, what is the least \n?\n\n\n.\n\n\nPermutations\n\n\nA permutations of a set \n is an ordered arrangement of elements of \n i.e. a sequence of every element in \n once.\n\n\nIn otherwords their is a bijection \n which matches some element in \n with another element in \n, creating a unique permutation, since it is one-to-one. So a set \n has a permutation \n.\n\n\nr-Permutation\n\n\nAn ordered sequence of \n distinct elements of \n, for example a 2-permutation of set \n would be \n or \n etc.\n\n\nIn otherwords their is a bijection \n which orders a subset of the elements in \n, creating a unique r-permutation. So an r-permutation would be \n.\n\n\nNumber of r-Permutations\n\n\nThe number of r-permutations will be:\n\n\n\n\nSince their are \n choises of what can go in the first position, and \n choises for the second, and so on. This simplifys to:\n\n\n\n\nFor the number of permutations we get:\n\n\n\n\nExample\n\n\nHow many permutations of the letters \nABCDEFGH\n contain \nABC\n as a consecutive substring.\n\n\nABCDEFGH\n = \nABC,D,E,F,G,H\n = \n\n\nStirling's approximation formula\n\n\n\n\nWe can show that its and approximation, by showing that it grows at the same rate as \n:\n\n\n\n\nIts often usefull to have lower and upper bounds:\n\n\n\n\nCombination\n\n\nAn unordered collection of elemenets of \n.\n\n\nThe number of combinations will be the number of permutations divided by the number of orderings which is \n.\n\n\n\n\nAproximations\n\n\nFrom Stirlings approximation formular, we can derive the following bounds:\n\n\n\n\nExample\n\n\nHow many diffrent 5 card poker hands can be dealt from 52 cards.\n\n\n\n\nIdentity\n\n\n\n\nProof:\n\n\n\n\nBinomial coeffiecients\n\n\nConsider:\n\n\n\n\nBy exanding the terms we can write it as a sum-of-monomials\n\n\n\n\nwere \n is the binomial coeffiects\n\n\n\n\nPascal's Identity\n\n\nFor all integers \n, and all integers \n, \n:\n\n\n\n\nProof:\n\n\nConsider the subset \n. Consider the subset \n where \n. Their are two cases\n\n\n\n\n\n\n\n\n\n\nTheir are:\n\n\n\n\n of the first kind, since \n was already \"choosen\" their are \n left to choose out of \n (not \n options).\n\n\n of the second kind, since no elements have been \"choosen\" their are \n left to choose out of \n (not \n options).\n\n\n\n\nThus it follows that:\n\n\n\nVandermonde's identity\n\n\nFor \n and \n:\n\n\n\n\nConsider two disjoint subsets \n and \n, with \n and \n, thus \n. Chosing r elements from \n, we get the following cases:\n\n\n\n\n elements from \n, \n elements from \n\n\n element from \n, \n elements from \n\n\n...\n\n\n elements from \n, \n elements from \n\n\n\n\nSo their is \n of kind k. So the sum of all cases is:\n\n\n\n\nCombinations with repetition\n\n\nA multi-set is an unordered collection of elements with possible repetition, were the size of a multi-set is the number of elements (including repetition). For example consider the multi-sets: \n and \n by definition \n.\n\n\nFor all integers \n, the number of r-combinations with repetitions, from a set \n of size \n is:\n\n\n\n\nWe can represent each r-Combination with repetition as a string of length \n with \n bars and \n stars. For example for the set \n, then we can map multi-sets to strings.\n\n\n\n\n is \n**|||**\n\n\n is \n*|***|*|\n\n\n is \n*|||**\n\n\n\n\nThus, the number of r-Combinations with repetition is equal to the number of strings of length \n with \n bars and \n stars which is:\n\n\n\n\nExample\n\n\nHow many solutions to \n are their.\n\n\nThis is equivilent to 11-combinations with repetion, where the set size is 3. Thus:\n\n\n\n\nMultinomial coefficients\n\n\nFor integers \n such that \n, let:\n\n\n\n\nFor all \n, and all \n:\n\n\n\n\nNote the binomial coefficent is the special case were \n\n\nExample\n\n\nHow many diffrent strings can be made from reordering the letters \"SUCCESS\".\n\n\n\n\nGraphs\n\n\n\n\nGraph\n\n\nA non-empty set of vertices (or nodes) and a set of edges that connect (pairs of) nodes\n\n\nUndirected graph\n\n\n\n\n\n\nTheir is at most one edge between distinct nodes\n\n\nNo self loops\n\n\nEdges have no direction\n\n\n\n\n\n\nDirected graphs\n\n\n\n\n\n\nAt most one directed edge from one not to another (so their can be 2 edges, one back and one forward)\n\n\nNo self loops\n\n\nOnly directed edges\n\n\n\n\n\n\n\n\nFormal defintion\n\n\nA directed graph \n consists of a non-empty set \n of vertices and a set \n of directed edges. Each edge \n has a start vertex \n and an end vertex \n. \n\n\nA undirected graph \n consits of a non-empty set \n of vertices and a set \n of undirected edges, where \n. Here we use \n to denote the \n-element subsets of \n.\n\n\nDegree and neighborhood\n\n\n\n\nDegree\n\n\nThe number of edges incident with it, denoted by \n.\n\n\nNeighborhood\n\n\nSet of vertices adjactent to \n denoted by \n.\n\n\n\n\nHandshanking theorem\n\n\nIf \n is an undirected graph with \n edges then:\n\n\n\n\nProof: Every edge contributes to the sum of the degrees twice since their it connects a pair of nodes.\n\n\nEven number of odd degree vertices\n\n\nTheir is always an even number of odd degree vertices.\n\n\nProof:\n\n\nFirst we use the handshaking lemma, and split the sum inot \n (odd degree vertices) and \n (even degree vertices).\n\n\n\n\nNext we subtract the sum of even vertices from both sides.\n\n\n\n\nNow we notice the lhs is even, since \n is even and the sum of even numbers is even. If the lhs is even the rhs must also be even, thus the sum of odd degree vertices is even.\n\n\nDirected graphs\n\n\n\n\nin-degree\n\n\nNumber of edges comming into \n, denoted by \n.\n\n\nout-degree\n\n\nNumber of edges comming out of \n, denoted by \n.\n\n\n\n\nSpecial graphs\n\n\n\n\nComplete graph\n\n\nDenoted by \n, a simple graph that contains one edge between each pair of distinct vertices.\n\n\nCycle\n\n\ntodo\n\n\nn-Cubes\n\n\nAn n-dimensional hypercube, or n-cube, is a graph with \n vertices representing all bit strings of lenth \n, where their is an edge between two vertices iff they differ in exactly one bit position.\n\n\nBipartite graphs\n\n\nA graph where it is possible to split vertices into two sets where all the edges go from one set to the other. An equivalent definition is that is is possible to color the vertices either red or blue so that no two adjacent vertices are the same color.\n\n\nComplete bipartite graph\n\n\nA bitite graph, dentoted by \n where their is \n vertices in one partition and \n veritces in the other partition, with every edge between the sets connected by an edge.\n\n\nSubgraph\n\n\nA subgraph of a graph \n is a graph \n where \n and \n.\n\n\nProper subgraph\n\n\nA subgraph which is not equal to the graph.\n\n\nInduced subgraph\n\n\nLet \n be a graph. The subgraph induced by a subset \n of the vertex set \n is the graph \n whos edge set \n contains an edge in \n iff both endpoints are in \n.\n\n\n\n\nBipartite grapgs matching\n\n\nBipartite graphs can be used to match elements in two sets, such as job assignments.\n\n\n\n\nMatching\n\n\nA subset of edges of graph \n such thet does not exsit two distinct edges in incident on the same vertex.\n\n\nMaximum matching\n\n\nA graph \n is a matching in \n iwth the maximum possible edges.\n\n\n\n\nHalls marriage theorem\n\n\nFor a bipartite graph \n with a bipartition \n, there exsists a matching \n that covers \n iff for all \n, \n.\n\n\nProof:\n\n\nFor \n with \n, let \n denote the neignbors of \n in \n. \n\n\nFirst \"only if\" direction:\n\n\n\n\nSupose there is a matching \n in \n that covers \n\n\nWe show that \n.\n\n\nSuppose that there is a subset \n such that \n.\n\n\nTheir is no matching \n that could cover by the pigeon hall theorem\n\n\n\n\n\"if\" direction:\n\n\n\n\nSupose \n.\n\n\nWe can prove a matching \n exsists which covers \n, by induction of size \n.\n\n\nBase case: \n. Since \n, their must be an edge covering the vertex \n in \n\n\nInductive step: By the inductive hypothasis, for a bipartite graph \n with \n. Suppose \n \n\n\nCase 1:\n\n\nFor every nonempty strict subset \n, we have \n. \n\n\nTake any \n with \n.\n\n\nRemove \n and \n and edges indicent on them from \n, resulting in a bipartite graph \n.\n\n\nBy the inductive hyptothesis, there must exist a matching \n in \n that covers \n since for every subset \n, \n.\n\n\nThus \n\n\nThus their is a matching \n\n\n\n\n\n\nCase 2:\n\n\nFor every nonempty strict subset \n, we have \n. \n\n\nAny matching that covers \n, must match \n to \n\n\nBy the induction hypothesis, their is a matching \n covering \n on the bipartite subgraph \n of \n induced by \n\n\nFurthemore the graph \n of \n induced by \n also satisfies the condition and containts a matching \n that covers \n since \n has \n\n\nThis implys \n, which violates the assumption about \n.\n\n\nLetting \n, \n defines a matching in \n that covers \n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerfect matching\n\n\nA bipartite graph \n with bipartition \n has a perfect matching if and only if \n and \n, \n.\n\n\nRepresenting graphs\n\n\n\n\nAdjeacency list\n\n\nSpecifying the vertices that are adjacent to each vertex. For directed graphs we list the outgoing vertex\n\n\nAdjaceny matrices\n\n\nA boolean matrix the rows and columns correspond to the vertexs in the graph. If their is an edge between them, that cell is true.\n\n\n\n\n\n\nAdjeacy list are better for sparce graphs since it uses less memory, adjacency matrixs use less memory for dence graph.\n\n\nAdjaceny matrices for multigraphs can use numbers instead to denote the number of connections between a pair\n\n\n\n\nIsomorphism of Graphs\n\n\nTwo (undirected) graphs \n and \n are \nisomorphic\n if their is a bijection \n, with the propery for all vertices \n i.e. the graphs are the same.\n\n\nTheir is no known \npolynomial time\n algorithum for finding isomorphism, instead we try to find an invarient between the graphs.\n\n\nPaths and Connectedness\n\n\n\n\nPath\n\n\nA sequence of edges connected diffrent vertices\n\n\n\n\nFor an undirected graph \n for path lenght \n, a path from \n to \n is a sequence:\n\n\n\n\nof interleaved verices \n and edges \n such that \n and \n and such that \n for all \n to \n.\n\n\n\n\nCycle\n\n\nA path that starts and ends at the same vertex.\n\n\nSimple\n\n\nA path is simple if it does not contain the same edge twice.\n\n\nTidy path\n\n\nA path were no vertex appears twice (except first and last).\n\n\nConnected\n\n\nTheir is a path between every pair of distinct vertices\n\n\n\n\nTheir is always a simple and tidy path between any pair of vertices of a connected undirected graph\n\n\nBy the definition of connectedness, for every pair of vertives \n,, their must exist a shortest oath \n in \n such that \n and \n. Suposse this path is not tidy and \n. // TODO: finish this\n\n\n\n\nConnected component\n\n\nA maximal connected subgraph\n\n\nStrongly connected\n\n\nFor every pair of vertices, their is a direct path from one to the other and visa-versa\n\n\nWeakly connected\n\n\nFor every pait of vertices, their is a path for the underlying undirected subgraph\n\n\nStrongly connect component\n\n\nMaximal strongly connected subgraph, that is not contained in a larger subgraph.\n\n\nDirected acyclic graph\n\n\nA directed graph with no circuits or loops\n\n\n\n\nEulers paths and curcuits\n\n\nDiscrete probability\n\n\n\n\nSample space\n\n\nAll possible outcomes, denoted by \n\n\n\n\nProbability distribution is a function \n, that assigns a probability to each item in the sample space, such that \n.\n\n\n\n\nEvent\n\n\nA subset of the possible outcomes\n\n\n\n\nBasic facts about probability of events\n\n\nTheorem\n\n\nSupose \n are sequence of pairwise disjoint events fromt he sample space \n.\n\n\n\n\nProof\n\n\nFollows from definitions\n\n\nDefinition\n\n\nLet \n be a probability distribution, and let \n be two events such that \n. The conditial probability of \n given \n denoted by \n is defined by\n\n\n\n\nIdependent events\n\n\nTwo events are indepenent if \n likewise \n.\n\n\nEvents \n are \npairwise independent\n if for every pair \n, \n and \n are indepenent.\n\n\nEvents \n are \nmutually independent\n if for every subset \n, \nERROR\n. \n\n\nBernoulli trails\n\n\nA probalistic experiment with two outcomes \n and \n. (Typically) diffrent trails are mutaully indepented (such as coin flips).\n\n\nBinomial distribution\n\n\nThe probability of \n successes in \n indepented bernoulli trials is\n\n\n\n\nThe binomial distribution is denoted by \n.\n\n\nRandom variable\n\n\nA function that assigns a real value to each outcome in a sample space.\n\n\nWe write \n as short hand \n.\n\n\nExample\n\n\nGiven a biased coin that lands heads with probabilty \n. What is the probabilty we flipped the coin \n times to get the first head.\n\n\n\n\nBayes theorem\n\n\nLet \n and \n be two events, from some sameple splace \n and \n a probability distribution of \n, such that \n and \n.\n\n\n\n\nGenralized bayes theorem\n\n\nLet \n be events that partition the sample space \n. Suppose \n and \n for all \n\n\n\n\nExpectation\n\n\n\n\nExpected value\n\n\nThe value of the random variable weighted by the probability of that outcome",
            "title": "Discrete Mathmatics and Mathmatical Reasoning"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#propositonal-logic",
            "text": "",
            "title": "Propositonal Logic"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#propositions",
            "text": "Proposition  A declarative sentence (declares a fact) that is either true or false.      Propositon  Not propositions      It is raining.  What is the data?",
            "title": "Propositions"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#logical-operators",
            "text": "From these propositions we can from new compound propositions with logical operators:     Operation  Example  Explanation      Negation   it is not the case that     Conjunction   both   and   must be true for   to be true    Disjunction   False when both   and   are false, true otherwise    Exclusive Or   Either   or   must be true ( not both ) for   to be true    Implication   False when   is true and   is false, true otherwise.   is the hypothesis/antecedent and   is the conclusion/conseqence    Biconditional    if and only if  , true when both   and   have the same truth values",
            "title": "Logical operators"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#truth-tables",
            "text": "With these logical operators we can build complex logical statements such as:  . To construct truth tables for these expressions we start with the individual variables   and  , and solve sub expressions until we reach the full expression.",
            "title": "Truth tables"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#operator-precedence",
            "text": "The order of which operators apply are as follows:     Operator  Precedence       1     2     3     4     5",
            "title": "Operator precedence"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#logical-equivalences",
            "text": "Tautology  A compound proposition that is  true , no matter what values the propositional variables hold.  Contradiction  A compound proposition that is  false , no matter what values the propositional variables hold.  Contingency  Compound propositions that are neither a tautology or a contradiction.     is a tautology   is a contradiction   is a contingency   Two compound propositions are  logically equivilent  if they have the same truth values in all cases. This occors when   is a tautology, where   and   are compound propositions.   is a statement, denoting that   and   are logically equivilent.",
            "title": "Logical Equivalences"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#important-equivalences",
            "text": "Equivalence  Name       Identity laws     Domination laws     Idempotent laws     Double negation law     Commutative laws     Associative laws     Distributive laws     De Morgan's laws     Absorption laws     Negation laws     Logical equivalences involving conditional statements     Logical equivalences involving biconditionals",
            "title": "Important Equivalences"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#notation",
            "text": "Simular to  , their exsists a notation for   and  .",
            "title": "Notation"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#constructing-new-logical-equivalences",
            "text": "Using equivalences we already know, we can prove expressions are tautologys or that two expressions are logically equivelent. While a truth table could also be used, it is much shorted to apply laws.   Show that   is a tautology.",
            "title": "Constructing new logical equivalences"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#satisfaction",
            "text": "Satisfiable  A compound proposition were there  is a  assignment of truth values that make it true.  Unsatisfiable  A compound proposition were there  is no  assignment of truth values that make it true.   The values which make a compound proposition true is called the  solution to the satisfiability problem .",
            "title": "Satisfaction"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#predicates-and-quantifiers",
            "text": "",
            "title": "Predicates and Quantifiers"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#predicates",
            "text": "Statements that involve variables such as   and   are undecided when their variables are not given a value. We need a more powerfull logic,  predicate logic , to model these statements.  We turn the statement   into the predicate   where   is  . When we give   a value, the statement becomes a proposition, e.g.   becomes   which is  .  A predicate can have any number of variables,  . A predicate with   variables is called an  -ary predicate.",
            "title": "Predicates"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#quantifiers",
            "text": "Assigning variables is not the only way to form a proposition from a predicate.  Quantification  expresses the extend to which a predicate is true over a range.     Name  Example  Note      Universal quantifier   is true if   is true for all x (domain)    Existential quantifier   is true if   is true for at least one value in the domain      Is   where the domain is all real numbers true.  No, if  ,  ) is false, thus not all elements in the domain make   true.",
            "title": "Quantifiers"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#restricting-the-domain",
            "text": "Consider   where the domain of   is  . We could express that as:   Their is a shorter notation for restricting the domain",
            "title": "Restricting the domain"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#precedence-of-quantifiers",
            "text": "and   have a higher precedence that all other logical operators.",
            "title": "Precedence of Quantifiers"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#binding-variables",
            "text": "When a quantifier is used on variable, or the variable is assigned a value, the variable is said to be  bound . Predicates must have all the variables bound to be turned into a proposition. Variables that are not bound are  free .  The  scope  of a quantifier is the part of the logical expression to which it is applied, hence variables outside of the scope of all quantifiers are free (if their value has not been assigned).",
            "title": "Binding Variables"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#logical-equivalence",
            "text": "Two expressions involving predicates and quantifiers are equal if their truth values are the same throughout the domain.   Show that   and   are logically equivalent.   Let   be some element in the domain, thus   must be  .  If   is  ,   and   must both be  .  Since   and   are   for all element in the domain,   must also be $\\top$.   So",
            "title": "Logical Equivalence"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#negation",
            "text": "Statement  Equivalent  Note        Their is an   for which   is       For every  ,   is       Prove     is  , if and only if   is   If   is  , then their is atleast one element in the domain for which   is true.  It follows then the",
            "title": "Negation"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#nested-quantifiers",
            "text": "More complex statements often involve more than one quantifier. For example the logical statement that  if   is positive and   is negative,   is negative  can be expressed as:",
            "title": "Nested Quantifiers"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#proof",
            "text": "",
            "title": "Proof"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#rules-of-inference",
            "text": "Proofs are mathmatical arguments that determine the truth of a statement.  By an argument , means a sequence of statements that end in a conclusion.  By valid , means the conclusion must follow from the arguments.  By premise , means the argument is valid if and only if it is impossible for all premises to be true and the conclusion false.  An  argument  is propositional logic is a sequence of propositons, where the final proposition is the conclusion and all others are the premises. An  argument form  in propositional logic is a sequence of compound propositions that is  valid  no matter what propositions are substitude for a premise, so long as the premises are true.  The  modus ponens  (law of detachment) is one rule we can use to proof statements.    Let   be \"it snows\" and   be \"will go skiing\", if \"we will go skiing if it snows\" is \"we will go skiing\" true?       Rule of Inference  Tautology  Name        Modus ponens      Modus tollens      Hypothetical syllogism      Disjunctive syllogism      Addition      Simplification      Conjunction      Resolution",
            "title": "Rules of Inference"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#using-rules-of-inference-to-build-arguments",
            "text": "Given the premesis:  ,  ,  ,   find an argument that shows the premisies lead to the conclusion  .",
            "title": "Using Rules of Inference to Build Arguments"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#fallacies",
            "text": "is a tautology, fallacies often look like tautologys but are just contingencies:   Fallacy of affirming the conclusion  occors when   is treated as a tautology, but it is false when   is false and   is true.  Fallacy of denying the hypothesis  occors when   is treated as a tautology, but it is false when   is false and   is true.",
            "title": "Fallacies"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#rules-of-inference-for-quantified-statements",
            "text": "Rule of Inference  Name  Notes       Universal instantiation   is true, where   is an element of the domain, since all elements are true.     Universal generalization   is true, if   (where   is an arbitary element in the domain) is true. Since we do not make any assumptions about  , other than its an element of the domain, all values of the domain must be true.     Existential instantiation  Their is an element   in the domain that makes   true. Note   is not arbitary since some elements in the domain do not make   true.     Existential generalization  If we know one element   in the domain that makes   true, then it follows that       Show that the premesis \"A student in the class has not read the book\" and \"Everyone in the class passed the exam\" implys the conclusion that \"Someone who passed the exam had not read the book\"   Let   be \"  is in the class\"  Let   be \"  passed the exam\"  Let   be \"  has read the book\"",
            "title": "Rules of Inference for Quantified Statements"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#sets-functions-relations-sequences-and-sums",
            "text": "",
            "title": "Sets, Functions, Relations, Sequences and Sums"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#set",
            "text": "Set  An unordered collection of elements",
            "title": "Set"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#set-operations",
            "text": "Name  Example      Equality     Membership     Non-Membership     Empty Set     Union     Intersection     Diffrence     Complement     Subset     Superset     Power Set   (set of all subsets)    Cardinality     Cartesian Product",
            "title": "Set Operations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#common-sets",
            "text": "Name  Symbol  Description      Booleans      Natural numbers      Integer      Positive Integers      Real Numbers      Rational      Complex Numbers",
            "title": "Common Sets"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#set-identitys",
            "text": "",
            "title": "Set Identitys"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#functions",
            "text": "",
            "title": "Functions"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#syntax",
            "text": "Function  A function   from set   to set   assigns an element from   to  , this is written as  .   Examples:   Ceil -   Floor -   Identity -   Factorial -       Type of function  Condition  Examples      Injective   ,     Surjective   ,     Bijection  Injective and surjective  ,",
            "title": "Syntax"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#composition",
            "text": "Let   and  . The composition function   is    The composition of two functions is a function  The composition of two injective functions is a injective function  The composition of two surjective functions is a surjective function  The composition of two bijections is a bijection",
            "title": "Composition"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#inverse",
            "text": "If   is a bijection, then the inverse of  , written   is   iff    Inverse of the identity function, is the identity function  Inverse of   is      and   is the identity function",
            "title": "Inverse"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#relations",
            "text": "A binary relation   on sets   and   is a subset    Often written as   for   A function   is a restricted relation where   For all a in   their exsists a   in   where (a, b) in relation   and   is unique      Given sets   a subset   is an  -ary relation.    where   is students and   is courses,",
            "title": "Relations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#relation-composition",
            "text": "Let   and  . The composition relation   is   The expression specifys the relations where   and   have a common  , thus   Closure   is a relation on  :    is the identity relation",
            "title": "Relation composition"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#types-of-relation",
            "text": "Type of relation  Condition  Example      Reflexive   ,     Symetric      Antisymetric   ,  ,     Transitive   ,  ,     Equivalance  Reflexive, symetric and transitive",
            "title": "Types of relation"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#equivalance-classes",
            "text": "Let   be an equivalence realtion on a set   and   let   be the equivalance class of a w.r.t  .  If   the   is called a representative of the equivalance class.  Let   be an equivalance realtion on   amd  . Then following statements are equivalnet",
            "title": "Equivalance classes"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#partitions-of-a-set",
            "text": "A partition of set   is a collection of disjoint, nonempty subsets that have   as their union, in other words the collection of subsets   with   (where   is the index set) forms a partition of   iff    for all    for all     From this can see:   If   is an equivalance class on  , then the equivalance classes of   form a partion of  .  Conversly, given a partition   of  , there exsists an equivalance relation   that has the sets  ,  , as its equivalence classes.",
            "title": "Partitions of a Set"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#sequences",
            "text": "Sequence  Ordered list of elements for example,   is   defines the sequence   Geometric progressions  A sequence in the form   Arithmetic progressions  A sequence in the form",
            "title": "Sequences"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#recurrence-relations",
            "text": "Recurrence relations  A recurance relations for   is an equation that expreses   in terms of one or more elements of     After 1 month a pair of rabbits is placed on an island  After every 2 months, each pair of rabbits produces a new pait of rabbits    The realtion is: ERROR  Solving a recurrence relations if finding the nth term, one way of solving is an iterative aproach   Supose a person deposits $1000 in savings with 3 percent intrest, how much is the account worth after 20 years   Let   denote the amount after   years",
            "title": "Recurrence relations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#sumations",
            "text": "Given a sequence  , the sum of the terms is",
            "title": "Sumations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#usefull-sumations",
            "text": "Sum  Closed form       ,",
            "title": "Usefull sumations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#product",
            "text": "Given a sequence  , the product of the terms is   or more generally for a finite index set  ,",
            "title": "Product"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#cartinality",
            "text": "Finite set  A set   is finite with cardinality   if there is a bijection from the set   to  .  Infinite set  A set that is not finite    Prove the set of natural numbers,  , is an infinite set.   Consider the function   defined as    is injective, since every element in the domain has an element in the codomain which is mapped to it.   is not subjective, since   is in the codomain but is has no element in the domain which maps to it.  Thus   is not a bijection.  Thus   is an infinite set.",
            "title": "Cartinality"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#countable-sets",
            "text": "Countable  A set that is either finite or has the same cardinality as the set of the positive integers  Uncountable  A set that is not countable   An infinite set   is countable, we denote the cardinality of   by      If   and   are countable sets, then   is also countable  If   is countable and for each   the set   is countable then   is countable    Show that the set of real numbers is uncountable    Assume that the set of real numbers is countable  The the real numbers between   and   must also be countable  We can list these numbers with decimal noation   Where   We can define a number   Where  ERROR  So   defines a number that is diffrent from   in the   number  Thus their is a real number between   and   that is not in the list  Therefore the set of real numbers cannot be listed.",
            "title": "Countable sets"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#schroder-bernstein-theorem",
            "text": "If   and   are sets with   and  , then  . In other words, if there are one-to-one functions   from   to   and   from   to  , then there is a one-to-one correspondence between   and  .   Show that    Since  , their is a one-to-one function (e.g. identity function) from   to   Lets define  , which is a mapping from   to  ,   so their is a one   to  .  Thus  , due to Schr\u00f6der-Bernstein Theorem",
            "title": "Schr\u00f6der-Bernstein Theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#cantors-theorem",
            "text": "Proof   Consider the injection   with   for any  .  Therefore   Assume a surjection   exsists  Let   Since   is a surjection, there must exist an   such that       If   then by definition of  ,  , thus   (contradiction)  If   then  , by definition of  ,   (contradiction)      One consequence of cantors theorem is that their is an infinite hierarchy of sets of larger cardinality.",
            "title": "Cantor\u2019s theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#induction",
            "text": "Mathematical Induction  To prove   is true for all positive integers, where   is a propositional function  Basic Step  Verifying   is true  Inductive step  Showing that the conditional   is true for all positive integers     Prove that if   is a positive integer    , thus   is true  Assuming   holds, then it must be shown that   holds   Thus if we can show  , then",
            "title": "Induction"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#strong-induction",
            "text": "Mathematical Strong Induction  To prove   is true for all positive integers, where   is a propositional function  Basic Step  Verifying   is true  Inductive step  Showing that the conditional   is true for all positive integers     Prove that if  , then   can be written as the product of primes    is true since   (  is a prime itself).   is either a prime or a composite number  If it is prime, then itself is a product of primes  If it is composite, then   is the product of two positive integers   and   that are  .  Thus since both are less than  , by strong induction they can be rewritten as the product of primes  Thus   can be written as the product of primes",
            "title": "Strong induction"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#multiplicative-inverses",
            "text": "Linear congruence  A congruence in the form   Inverse  An interger  , for which    Their is an efficent algorithum for computing the inverse of   when  . We can find the linear combination  , reducing both sides by modulo   gives   as an inverse.",
            "title": "Multiplicative Inverses"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example",
            "text": "Find an inverse of   modulo     Thus   is the inverse of",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#the-chinese-remainder-theorem",
            "text": "Let   be pairwise relative primes greater than 1.   Then the system above has a unique solution modulo",
            "title": "The chinese remainder theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_1",
            "text": "",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#fermats-little-theorem",
            "text": "If   is prime and   is an integer not divisible by   then:",
            "title": "Fermat's little theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_2",
            "text": "Find  .   and  , so by fermat's little theorem  .   for every positive integer  . Therefore  .  . Hence  .",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#counting",
            "text": "",
            "title": "Counting"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#product-rule",
            "text": "If   and   are finite sets   where",
            "title": "Product rule"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_3",
            "text": "How many diffrent car license plates can be made of 3 uppercase letters and 3 digits.",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#counting-subsets",
            "text": "A finite set   has   distinct subsets.",
            "title": "Counting subsets"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#proof_1",
            "text": "Theis is bijection between the bit strings of length  , since an arbitary bit string tells us what elements from set   is in the subset. Thus by the product rule their are   distinct subsets.",
            "title": "Proof"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#counting-functions",
            "text": "For all finite sets   and  , the number of distinct functuons   is",
            "title": "Counting functions"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#proof_2",
            "text": "Their is a bijection between between functions   and strings of length   over an alphabet  . This is because each string gives all the values of a function  .",
            "title": "Proof"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#sum-rule",
            "text": "If   and   are disjoint then  .",
            "title": "Sum rule"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_4",
            "text": "Suppose variables can be 1 uppercase letter or 1 uppercase letter and a number  , since the sets are disjoint.",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#subtraction-rule",
            "text": "For sets   and  ,  .",
            "title": "Subtraction rule"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_5",
            "text": "How many bit strings of lenghth 8 either start with a 1 bit, or end with two 00 bits.",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#the-pigeonhole-principle",
            "text": "For any positive integer  , if   objects, are placed in   boxes,then at least one box contains two or more objects.  Or more formally, if   maps finite set   with",
            "title": "The pigeonhole principle"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#generalized-pigeon-hole",
            "text": "If   objects placed in   boxes, then at least one box contains at least  .",
            "title": "Generalized pigeon hole"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_6",
            "text": "Atleast   students were born in the dame month, what is the least  ?  .",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#permutations",
            "text": "A permutations of a set   is an ordered arrangement of elements of   i.e. a sequence of every element in   once.  In otherwords their is a bijection   which matches some element in   with another element in  , creating a unique permutation, since it is one-to-one. So a set   has a permutation  .",
            "title": "Permutations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#r-permutation",
            "text": "An ordered sequence of   distinct elements of  , for example a 2-permutation of set   would be   or   etc.  In otherwords their is a bijection   which orders a subset of the elements in  , creating a unique r-permutation. So an r-permutation would be  .",
            "title": "r-Permutation"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#number-of-r-permutations",
            "text": "The number of r-permutations will be:   Since their are   choises of what can go in the first position, and   choises for the second, and so on. This simplifys to:   For the number of permutations we get:",
            "title": "Number of r-Permutations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_7",
            "text": "How many permutations of the letters  ABCDEFGH  contain  ABC  as a consecutive substring.  ABCDEFGH  =  ABC,D,E,F,G,H  =",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#stirlings-approximation-formula",
            "text": "We can show that its and approximation, by showing that it grows at the same rate as  :   Its often usefull to have lower and upper bounds:",
            "title": "Stirling's approximation formula"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#combination",
            "text": "An unordered collection of elemenets of  .  The number of combinations will be the number of permutations divided by the number of orderings which is  .",
            "title": "Combination"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#aproximations",
            "text": "From Stirlings approximation formular, we can derive the following bounds:",
            "title": "Aproximations"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_8",
            "text": "How many diffrent 5 card poker hands can be dealt from 52 cards.",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#identity",
            "text": "Proof:",
            "title": "Identity"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#binomial-coeffiecients",
            "text": "Consider:   By exanding the terms we can write it as a sum-of-monomials   were   is the binomial coeffiects",
            "title": "Binomial coeffiecients"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#pascals-identity",
            "text": "For all integers  , and all integers  ,  :   Proof:  Consider the subset  . Consider the subset   where  . Their are two cases      Their are:    of the first kind, since   was already \"choosen\" their are   left to choose out of   (not   options).   of the second kind, since no elements have been \"choosen\" their are   left to choose out of   (not   options).   Thus it follows that:",
            "title": "Pascal's Identity"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#vandermondes-identity",
            "text": "For   and  :   Consider two disjoint subsets   and  , with   and  , thus  . Chosing r elements from  , we get the following cases:    elements from  ,   elements from    element from  ,   elements from   ...   elements from  ,   elements from    So their is   of kind k. So the sum of all cases is:",
            "title": "Vandermonde's identity"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#combinations-with-repetition",
            "text": "A multi-set is an unordered collection of elements with possible repetition, were the size of a multi-set is the number of elements (including repetition). For example consider the multi-sets:   and   by definition  .  For all integers  , the number of r-combinations with repetitions, from a set   of size   is:   We can represent each r-Combination with repetition as a string of length   with   bars and   stars. For example for the set  , then we can map multi-sets to strings.    is  **|||**   is  *|***|*|   is  *|||**   Thus, the number of r-Combinations with repetition is equal to the number of strings of length   with   bars and   stars which is:",
            "title": "Combinations with repetition"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_9",
            "text": "How many solutions to   are their.  This is equivilent to 11-combinations with repetion, where the set size is 3. Thus:",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#multinomial-coefficients",
            "text": "For integers   such that  , let:   For all  , and all  :   Note the binomial coefficent is the special case were",
            "title": "Multinomial coefficients"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_10",
            "text": "How many diffrent strings can be made from reordering the letters \"SUCCESS\".",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#graphs",
            "text": "Graph  A non-empty set of vertices (or nodes) and a set of edges that connect (pairs of) nodes  Undirected graph    Their is at most one edge between distinct nodes  No self loops  Edges have no direction    Directed graphs    At most one directed edge from one not to another (so their can be 2 edges, one back and one forward)  No self loops  Only directed edges",
            "title": "Graphs"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#formal-defintion",
            "text": "A directed graph   consists of a non-empty set   of vertices and a set   of directed edges. Each edge   has a start vertex   and an end vertex  .   A undirected graph   consits of a non-empty set   of vertices and a set   of undirected edges, where  . Here we use   to denote the  -element subsets of  .",
            "title": "Formal defintion"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#degree-and-neighborhood",
            "text": "Degree  The number of edges incident with it, denoted by  .  Neighborhood  Set of vertices adjactent to   denoted by  .",
            "title": "Degree and neighborhood"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#handshanking-theorem",
            "text": "If   is an undirected graph with   edges then:   Proof: Every edge contributes to the sum of the degrees twice since their it connects a pair of nodes.",
            "title": "Handshanking theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#even-number-of-odd-degree-vertices",
            "text": "Their is always an even number of odd degree vertices.  Proof:  First we use the handshaking lemma, and split the sum inot   (odd degree vertices) and   (even degree vertices).   Next we subtract the sum of even vertices from both sides.   Now we notice the lhs is even, since   is even and the sum of even numbers is even. If the lhs is even the rhs must also be even, thus the sum of odd degree vertices is even.",
            "title": "Even number of odd degree vertices"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#directed-graphs",
            "text": "in-degree  Number of edges comming into  , denoted by  .  out-degree  Number of edges comming out of  , denoted by  .",
            "title": "Directed graphs"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#special-graphs",
            "text": "Complete graph  Denoted by  , a simple graph that contains one edge between each pair of distinct vertices.  Cycle  todo  n-Cubes  An n-dimensional hypercube, or n-cube, is a graph with   vertices representing all bit strings of lenth  , where their is an edge between two vertices iff they differ in exactly one bit position.  Bipartite graphs  A graph where it is possible to split vertices into two sets where all the edges go from one set to the other. An equivalent definition is that is is possible to color the vertices either red or blue so that no two adjacent vertices are the same color.  Complete bipartite graph  A bitite graph, dentoted by   where their is   vertices in one partition and   veritces in the other partition, with every edge between the sets connected by an edge.  Subgraph  A subgraph of a graph   is a graph   where   and  .  Proper subgraph  A subgraph which is not equal to the graph.  Induced subgraph  Let   be a graph. The subgraph induced by a subset   of the vertex set   is the graph   whos edge set   contains an edge in   iff both endpoints are in  .",
            "title": "Special graphs"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#bipartite-grapgs-matching",
            "text": "Bipartite graphs can be used to match elements in two sets, such as job assignments.   Matching  A subset of edges of graph   such thet does not exsit two distinct edges in incident on the same vertex.  Maximum matching  A graph   is a matching in   iwth the maximum possible edges.",
            "title": "Bipartite grapgs matching"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#halls-marriage-theorem",
            "text": "For a bipartite graph   with a bipartition  , there exsists a matching   that covers   iff for all  ,  .  Proof:  For   with  , let   denote the neignbors of   in  .   First \"only if\" direction:   Supose there is a matching   in   that covers   We show that  .  Suppose that there is a subset   such that  .  Their is no matching   that could cover by the pigeon hall theorem   \"if\" direction:   Supose  .  We can prove a matching   exsists which covers  , by induction of size  .  Base case:  . Since  , their must be an edge covering the vertex   in   Inductive step: By the inductive hypothasis, for a bipartite graph   with  . Suppose     Case 1:  For every nonempty strict subset  , we have  .   Take any   with  .  Remove   and   and edges indicent on them from  , resulting in a bipartite graph  .  By the inductive hyptothesis, there must exist a matching   in   that covers   since for every subset  ,  .  Thus   Thus their is a matching     Case 2:  For every nonempty strict subset  , we have  .   Any matching that covers  , must match   to   By the induction hypothesis, their is a matching   covering   on the bipartite subgraph   of   induced by   Furthemore the graph   of   induced by   also satisfies the condition and containts a matching   that covers   since   has   This implys  , which violates the assumption about  .  Letting  ,   defines a matching in   that covers  .",
            "title": "Halls marriage theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#perfect-matching",
            "text": "A bipartite graph   with bipartition   has a perfect matching if and only if   and  ,  .",
            "title": "Perfect matching"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#representing-graphs",
            "text": "Adjeacency list  Specifying the vertices that are adjacent to each vertex. For directed graphs we list the outgoing vertex  Adjaceny matrices  A boolean matrix the rows and columns correspond to the vertexs in the graph. If their is an edge between them, that cell is true.    Adjeacy list are better for sparce graphs since it uses less memory, adjacency matrixs use less memory for dence graph.  Adjaceny matrices for multigraphs can use numbers instead to denote the number of connections between a pair",
            "title": "Representing graphs"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#isomorphism-of-graphs",
            "text": "Two (undirected) graphs   and   are  isomorphic  if their is a bijection  , with the propery for all vertices   i.e. the graphs are the same.  Their is no known  polynomial time  algorithum for finding isomorphism, instead we try to find an invarient between the graphs.",
            "title": "Isomorphism of Graphs"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#paths-and-connectedness",
            "text": "Path  A sequence of edges connected diffrent vertices   For an undirected graph   for path lenght  , a path from   to   is a sequence:   of interleaved verices   and edges   such that   and   and such that   for all   to  .   Cycle  A path that starts and ends at the same vertex.  Simple  A path is simple if it does not contain the same edge twice.  Tidy path  A path were no vertex appears twice (except first and last).  Connected  Their is a path between every pair of distinct vertices   Their is always a simple and tidy path between any pair of vertices of a connected undirected graph  By the definition of connectedness, for every pair of vertives  ,, their must exist a shortest oath   in   such that   and  . Suposse this path is not tidy and  . // TODO: finish this   Connected component  A maximal connected subgraph  Strongly connected  For every pair of vertices, their is a direct path from one to the other and visa-versa  Weakly connected  For every pait of vertices, their is a path for the underlying undirected subgraph  Strongly connect component  Maximal strongly connected subgraph, that is not contained in a larger subgraph.  Directed acyclic graph  A directed graph with no circuits or loops",
            "title": "Paths and Connectedness"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#eulers-paths-and-curcuits",
            "text": "",
            "title": "Eulers paths and curcuits"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#discrete-probability",
            "text": "Sample space  All possible outcomes, denoted by    Probability distribution is a function  , that assigns a probability to each item in the sample space, such that  .   Event  A subset of the possible outcomes",
            "title": "Discrete probability"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#basic-facts-about-probability-of-events",
            "text": "",
            "title": "Basic facts about probability of events"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#theorem",
            "text": "Supose   are sequence of pairwise disjoint events fromt he sample space  .",
            "title": "Theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#proof_3",
            "text": "Follows from definitions",
            "title": "Proof"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#definition",
            "text": "Let   be a probability distribution, and let   be two events such that  . The conditial probability of   given   denoted by   is defined by",
            "title": "Definition"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#idependent-events",
            "text": "Two events are indepenent if   likewise  .  Events   are  pairwise independent  if for every pair  ,   and   are indepenent.  Events   are  mutually independent  if for every subset  ,  ERROR .",
            "title": "Idependent events"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#bernoulli-trails",
            "text": "A probalistic experiment with two outcomes   and  . (Typically) diffrent trails are mutaully indepented (such as coin flips).",
            "title": "Bernoulli trails"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#binomial-distribution",
            "text": "The probability of   successes in   indepented bernoulli trials is   The binomial distribution is denoted by  .",
            "title": "Binomial distribution"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#random-variable",
            "text": "A function that assigns a real value to each outcome in a sample space.  We write   as short hand  .",
            "title": "Random variable"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#example_11",
            "text": "Given a biased coin that lands heads with probabilty  . What is the probabilty we flipped the coin   times to get the first head.",
            "title": "Example"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#bayes-theorem",
            "text": "Let   and   be two events, from some sameple splace   and   a probability distribution of  , such that   and  .",
            "title": "Bayes theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#genralized-bayes-theorem",
            "text": "Let   be events that partition the sample space  . Suppose   and   for all",
            "title": "Genralized bayes theorem"
        },
        {
            "location": "/Discrete-Mathmatics-and-Mathmatical-Reasoning/#expectation",
            "text": "Expected value  The value of the random variable weighted by the probability of that outcome",
            "title": "Expectation"
        },
        {
            "location": "/Inf2A - Formal Languages/",
            "text": ".latex-box.math-false {text-align: center;}\n.math-true {vertical-align: middle;}\n\n\n\nInf2A - Formal Languages\n\n\nClosure Properties of Regular Languages\n\n\n-NFA\n\n\nIf we allow \n transitions, which represents an instananious transition, NFA's can be transformed to have one start and one accepting state.\n\n\nThis doesnt increase the power of NFA's, but it has some convenience.\n\n\nConcatination\n\n\n\n\n is the concatination of two languages, for example \n, \n becomes \n\n\nIts obvious the concatination is closed, since we can add an \n transition between the start and accepting states of the machines for languages \n and \n\ns\n\n\nKleene star\n\n\n\n\n Is the langauge of 0 or more strings of \n\n\nAgain kleene start is also closed. By introducing a new start state (which is also an accepting state) with an \n transition to the machine for \n and add \n transitions back from the machine to the new start/accepting state. This allows the machine for \n to run for any amount of times.\n\n\nRegular Expressions\n\n\nRegular expressions are a language for defining languages.\n\n\n\n\n\n\n\n\nSymbol\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n (for \n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrder\n\n\nIn the absence of brackets the order of operation is as follows:\n\n\n\n\n\n\n\n\n\n\n\n\nKleene's Theorem\n\n\nKleene's Theorem:\n DFA's and regular expressions give rise to exactly the same class of langauges (the regular langauges)\n\n\nSo regular languages can be defined as regular expressions.\n\n\nKleene algebra\n\n\nRegular expressions can be manipulated with kleene algebra without changing the language\n\n\n\n\n\n\n\n\n...\n\n\n\n\n\n\nArdens rule\n\n\nGiven an equation of the form \n its smallest solution is \n. If \n, this is the only solution\n\n\nDFA's to regular expressions\n\n\nFor each state \n, the variable \n stands for the set of states that take us from \n to the accepting state.\n\n\n\n\nApplications of regular languages\n\n\nGrep\n\n\nSearch for prices in pounds and pence:\n\n\ngrep \"[0-9]*\\.[0-9][0-9]\" document.txt\n\n\n\n\n\n\nfgrep\n searches for one or more fixed string, using an efficent string matching algorthum\n\n\ngrep\n searches for strings matching a pattern\n\n\negrep\n searches for strings matching an extended pattern\n\n\n\n\nString searching\n\n\nSupose we want to search for string \n in document \n. An efficent implentation is the Knuth-Morris-Pratt algorithum:\n\n\n\n\nConvert NFA accepting \n     to DFA M (costs some time, worth it for short \n and long \n).\n\n\nRun \n through \n (each character in \n processed once, no buffering).\n\n\nEvery time we get to the accepting state of \n, signal a hit.\n\n\n\n\nThis can be extended to search for mulitple strings in parralel with a more complex NFA.\n\n\nPattern searching\n\n\negrep\n will print all lines containing a match.\n\n\n\n\nWe can convert a pattern into a (smallish) NFA.\n\n\nWe can run the NFA using the just-in-time simulation (avoids exponential state-space blow-up).\n\n\n\n\nData validation\n\n\n\n\nWithin XML we can enforce constraints on parts of the data.\n\n\nFor text fields on web froms, we can check if the input text is in the correct form.\n\n\n\n\nLexing\n\n\nEven tough a higher level language is to complex to be regular, regular expressions can be used to identify the fundemental building blocks (tokens) of the language.\n\n\n\n\nLexical Analysis\n\n\nBreaking up source code into a series of tagged tokens (lexemes)\n\n\nLexical class\n\n\nThe class of a token, for example \n1000\n would be an integer literal, \nfoobar\n would be an identifier.\n\n\n\n\nHow lexers work\n\n\n\n\nBuild NFA's for the lexical classes \n (in order of priority)\n\n\nRun the 'parrallel' automaton \n until it expires\n\n\nThe last point in which we were in an accepting state is the largest match, chose the smallest \n such that were in an accepting state of \n. Chose class \n as the lexical class for \n which is the highest priorty.\n\n\nPerform the specified action for the class \n\n\n\n\nVerification\n\n\nRegulare langauge theory can help verify desirable properites:\n\n\n\n\nSaftey, i.e. \nbad things dont happen\n\n\nLiveness, i.e. \ngood things do happen\n\n\nFierness, i.e. \nthings good for some processes dont cause to much badness to others\n\n\n\n\nExample\n\n\nSuppose we have two processes \n that have use of a shared resource, but must not be given access at the same time.\n\n\n can comunicate using three shared flags\n- \nreq0\n initially false, whether \n wants access\n- \nreq1\n initially false, whether \n wants access\n- \nturn\n who is being allowed a turn\n\n\nCode for \n when it wants access\n\n\nreq0 = true\nturn = 1\nwhile(req1 && turn == 1) WAIT\n// P0 can now access resource\nreq0 = false\n\n\n\n\nCode for \n is the same with \n swapped and \nreq0\n and \nreq1\nswapped.\n\n\nThis can be moddled by a finite state machine:\n\n\nThe language for the complete system can now be obtained:\n\n\nwhere \n is the interleaving of regular langauges.\n\n\nNow machine \n with 200 states can be built.\n\n\nNow we can verify\n- Mutual exclusion: \n and \n can never access simulaneously.\n- Progress\n\n\nThe Pumping Lemma\n\n\nLoops in DFA's\n\n\nA machine \n with states \n will always have a loop since we can process strings \n where \n, thus we visit some state twice. Thus any string \n can be decomposed into:\n- \n, the prefix of \n that leads to the first visit of the repeated state \n\n- \n, the loop which goes from \n to \n\n- \n, whatever is left of \n after \n\n\nLemma\n\n\nSuppose \n is a regular language, then \n has the following property \n:\n\n\nTheir exists \n such that, for all strings \n with \n and \n, there exist strings \n such that \n, \n, and for every \n we have \n \n\n\nContrapositive\n\n\nTo prove a language is not reqular, we take the contrapositive of the pumping lemma \n:\n\n\nFor all \n, there exist strings \n with \n and \n such that, for every decomposition of \n as \n where \n, their is some \n which \n\n\nUsing the pumping lemma\n\n\n\n\nYour argument must work for all \n.\n\n\nChoose strings \n (which might depend on \n) to satisfy \n and \n. Additionaly \n should \"disallow pumping\" (number of loops depend on language).\n\n\nYour argument must work for all decompositions of \n as \n with \n.\n\n\nChoose the number \n such that \n, here \n might depend on all the previous data.\n\n\n\n\nExample 1\n\n\n\n\nShow that \n is not regular\n\n\n\n\nSuppose \n\n\nLet \n, \n, \n\n\nLet \n\n\n for some \n\n\nThus \n\n\n, thus \n satisfys \n, thus it is not regular\n\n\n\n\n\n\nExample 2\n\n\n\n\nShow that \n is not regular\n\n\n\n\nSuppose \n\n\nLet \n, \n, \n so \n\n\nGiven \n where \n, \n (since \n is non empry and the length of \n is length \n)\n\n\nLet \n, thus \n, \n\n\nThe length of the new string becomes \n\n\nThe next perfect string after \n is \nk^2 + 2k + 1\nk^2\n(k+1)^2 = \n\n\nThus \n isnt a perfect square, thus \n\n\nThus \n satisfys \n, thus it is not regular\n\n\n\n\n\n\nContext free languages\n\n\nContext free grammers\n\n\n\n\nThis grammer generates simple arithmetic expressions such as \n and \n.\n\n\n\n\nTerminals\n\n\nThe ultimate constituants of the phrases generated\n\n\nNon-terminals\n\n\nDesignate varoious sub-phrases, such as \nExp\n, \nNum\n etc\n\n\nProductions\n\n\nDefines how a non-terminal is expanded into more terminals/non-terminals\n\n\nStart symbol\n\n\nNon-terminal to start with, in this case \nExp\n\n\nSyntax tree\n\n\nTree formed starting from the start symbol we grow syntax trees my expanding the non-terminals using the grammer rules.\n\n\nStructural ambiguity\n\n\nStrings that have multiple possible syntax trees\n\n\n\n\nThe language generated by a grammer is the set of \nstrings of terminals\n that can be derived.\n\n\nDerivations\n\n\nAt each stage we choose a non terminal and expand using a suitable rule, when their are no terminals left a derivation is complete.\n\n\n\n\n\n\nLeftmost derivation\n\n\nDerivation expanding the leftmost terminal each time\n\n\nRightmost derivation\n\n\nDerivation expanding the rightmost terminal each time\n\n\n\n\nFormal defintion\n\n\nA context free grammer (CFG) \n consists of:\n\n\n\n\nA finite set of \n non terminals\n\n\nA finitie set of \n terminals, disjoint from \n\n\nA finite set \n of productions of the form \n, where \n\n\nA choise of start symbol \n \n\n\n\n\nRegular languages to context free\n\n\n\n\nInput symbols becomes termials\n\n\nStates become non-terminals\n\n\nInitial state becomes start state\n\n\nEvery transition \n becomes \n\n\nAccepting states \n becomes \n\n\n\n\nConsider the following FSM.\n\n\nTODO: insert FSM\n\n\nThe context free form is\n\n\n\n\nPushdown Automata\n\n\n\n\nPushdown Automata\n\n\nA control unit with finite states, equipped with a read head (that can only read the input left to right). Additionaly their is a stack which can be read, pushed and popped.\n\n\n\n\nConsider a PDA with a single state \n, input alphabet \n and stack alphabet \n. Call \n the initial stack symbol.\n\n\nThe DA has four transitions in the form \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor example, if the current read symbol is \n and the current stack symobl is \n, pop \n then push \n then \n\n\n\n\n\n\n\n\nTransition\n\n\nInput\n\n\nStack state\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\nIf the input string has well matched brackets the stack will be emptyed when the input string ends, otherwise the brackets are not well matched.\n\n\nFormal defintiion\n\n\n\n\nA finite set of control states \n\n\nFintite set of input alphabet \n\n\nFinite stack alphabet \n including start symbol \n\n\nStart state \n\n\nFinite transition relation \n\n\n\n\nA string is accepted if their is some run of \n on \n starting at \n with stack \n, and finishing (at any control state) with an empty stack having consumed all of \n.\n\n\nCGG's to NPDA's\n\n\n\n\nStingle state \n, input alphabet \n, and stack alphabet \n, and \n as the initial stack symbol\n\n\nFor each production \n in P, include an \n transition, \n\n\nFor each terminal \n, include a transition \n\n\n\n\nConsider parsing the string \n using the CFG earlier. At each stage we guess the correct rule to apply.\n\n\n\n\nNote that each stage, the combination of input read and stack state gives a \nsentential form\n. The result is the leftmost derivation.\n\n\nLL(1) Pridictive Parsing\n\n\n\n\nLL(1)\n\n\nRead the input from the \nL\neft, build the \nL\neftmost derivation, looking \n1\n symbol ahead.\n\n\n\n\nTo determine what rules to apply (not guess as we have done before) we use two pieces of infomation:\n\n\n\n\nCurrent lexical token\n\n\nThe nonterminal to be expanded\n\n\n\n\nIf it is always possible to determine the rule using these pieces of infomation the languages is said to be LL(1).\n\n\nExample\n\n\nConsider the following CFG\n\n\n\n\nWe can derive a \nparse table\n, which is a table of terminals and nonterminals that tells us what rule to apply for each case.\n\n\n\n\nThe \n signifys the end of input. Blank entries corresponds to illegals states thus the string is illegal.\n\n\nParsing\n\n\nGiven a parse table we can parse a string using the following:\n\n\n\n\nBegin with the start symbol \n on the stack.\n\n\nIf the current input symbol \n (could be \n), and the current stack symbol is a non-terminal \n, look up the rule for \n in the table.\n\n\nIf the current input symbol is \n and the stack symbol is \n just pop \n and advance input.\n\n\nAccept if the stack empties with \n as the input symbol.\n\n\n\n\nExample\n\n\nConside the input string \n and the parse table from above:\n\n\n\n\nParsing is as follows:\n\n\n\n\nFormal defintion\n\n\n is LL(1) if for each terminal \n and nonterminal \n, there is some production \n with the following property:\n\n\nIf \n is a sentential form appearing in a leftmost derivation of some string \n, the next sentential form appearing in the derivation is necessarily \n.\n\n\nNon-LL(1) grammers\n\n\nConsider the folloing grammer:\n\n\n\n\nNow suppose the initial stack is \n and the first input symbol is \n. We cant decide without looking ahead, if the rule to apply is \n or \n.\n\n\nGenerating parse tables\n\n\n\n\n\n\nThe set of all terminals that can appear at the start of a phrase derived from \n.\n\n\n\n\nThe set of all terminals that can appear immediately after X in some sentential form derived from the start symbol \n.\n\n\ncan be empty\n\n\n\n\ncan begin with \n\n\n, and \n\n\n\n\n\n\nFor each nonterminal \n, compute two sets called \n and \n.\n\n\nFirst sets\n\n\nCompute set \n, which is the set of nonterminals that 'can be \n'.\n\n\nAdd \n to \n whenever \n is a production \n.\n\n\nIf \n is a production and all \n are already in \n, add \n to \n.\n\n\nRepeat until \n stabilizes\n\n\n\n\n\n\nSet \n for each \n.\n\n\nFor each nonterminal \n, initially set \n to \n if \n, or \n otherwise.\n\n\nFor each production \n and each \n, if \n and \n, add \n to \n.\n\n\nRepeat step (d.) until all \n sets stabalize \n\n\n\n\n\n\nFollow sets\n\n\nIntially set \n for the start symbol \n, and \n for all nonterminals \n.\n\n\nFor each production \n, each splitting of \n as \n where \n 1, and each \n with \n, add all of \n (excluding \n) to \n.\n\n\nFor each production \n and each splitting of \n as \n or \n with \n, add all of \n to \n.\n\n\nRepeat step 3 until all \n sets stabalize.\n\n\n\n\n\n\n\n\n\n\nUse the First and Follow sets to fill out the parse tables.\n\n\nFor each production \n of \n in turn:\n\n\nFor each terminal \n, if \n \ncan begin with\n \n, insert \n in row \n, column \n.\n\n\nIf \n \ncan be empty\n, then for each \n (where b may be \n), insert \n in row \n, column \n.\n\n\nIf their are clashes, the grammer is not LL(1).\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\nConsider the following CFG\n\n\n\n\n\n\nFirst sets\n\n\n\n\n\n\n\n\n (since \n)\n\n\n (since \n)\n\n\n (since \n)\n\n\n (since \n) \n\n\n\n\n\n\nFollow sets\n\n\nStart with \n\n\n\n\n\n\n\n\n\n\n\n\nFor production \n\n\n \ncan be empty\n \n\n\nInsert the production into \n and \n\n\n\n\n\n\nFor production \n\n\n can begin with \n\n\nInsert the production into \n\n\n\n\n\n\nFor prodcution \n\n\n can begin with \n\n\nInsert the production into \n\n\n\n\n\n\n\n\n\n\nProblems with grammers\n\n\n\n\nThey may be ambiguous (can have more than one leftmost derivation)\n\n\nThey may have shared prefixes\n\n\nThey may be left-recursive e.g. \n\n\n\n\nAmbiguity\n\n\nHere is an example of an ambigious statement:\n\n\n\n\nFor example, \n, could be parsed as \n or \n.\n\n\nTo make it \nunambiguous\n, we can add nonterminals, to capture distinct classes of expressions\n\n\n\n\nNote this is note quite LL(1) yet.\n\n\nShared Prefixs\n\n\n\n\nIn this example, an LL(1) parser would have no way to choose between these rules. The solution is to 'factor out' the common parts to delay the choice.\n\n\n\n\nLeft recursion\n\n\nConside the rules\n\n\n\n\nNotice how it must start with \n, and that it is followed by zero or more \n or \n. We can use this observation to remove the left recursion like so:\n\n\n\n\nChomsky Normal Form\n\n\nA context free grammer \n is in \nChomsky normal form (CNF)\n if all productions are of the form:\n\n\n\n\nIf we disregard the empty string every language \n can be transformed into a grammer \n in CFG\n\n\n\n\nConverting to CFG\n\n\n\n\nRemove all \n productions, and for every rule \n where \n can be empty, add a new rule \n\n\nRemove 'unit' productions \n.\n\n\nFor each terminal, introduce a nonterminal \n, for all rules \n where \n. Replace each \n with \n\n\nFor every production \n with \n, add new symbols \n and replace the rules with \n.\n\n\n\n\nExample\n\n\nConsider the grammer\n\n\n\n\n\n\nRemove \n productions\n\n\n\nRemove 'unit productions' \n\n\n\n\nAdd \n productions\n\n\n\nSplit long productions",
            "title": "Inf2A   Formal Languages"
        },
        {
            "location": "/Inf2A - Formal Languages/#inf2a-formal-languages",
            "text": "",
            "title": "Inf2A - Formal Languages"
        },
        {
            "location": "/Inf2A - Formal Languages/#closure-properties-of-regular-languages",
            "text": "",
            "title": "Closure Properties of Regular Languages"
        },
        {
            "location": "/Inf2A - Formal Languages/#-nfa",
            "text": "If we allow   transitions, which represents an instananious transition, NFA's can be transformed to have one start and one accepting state.  This doesnt increase the power of NFA's, but it has some convenience.",
            "title": "-NFA"
        },
        {
            "location": "/Inf2A - Formal Languages/#concatination",
            "text": "is the concatination of two languages, for example  ,   becomes   Its obvious the concatination is closed, since we can add an   transition between the start and accepting states of the machines for languages   and  \ns",
            "title": "Concatination"
        },
        {
            "location": "/Inf2A - Formal Languages/#kleene-star",
            "text": "Is the langauge of 0 or more strings of   Again kleene start is also closed. By introducing a new start state (which is also an accepting state) with an   transition to the machine for   and add   transitions back from the machine to the new start/accepting state. This allows the machine for   to run for any amount of times.",
            "title": "Kleene star"
        },
        {
            "location": "/Inf2A - Formal Languages/#regular-expressions",
            "text": "Regular expressions are a language for defining languages.     Symbol  Definition               (for  )",
            "title": "Regular Expressions"
        },
        {
            "location": "/Inf2A - Formal Languages/#order",
            "text": "In the absence of brackets the order of operation is as follows:",
            "title": "Order"
        },
        {
            "location": "/Inf2A - Formal Languages/#kleenes-theorem",
            "text": "Kleene's Theorem:  DFA's and regular expressions give rise to exactly the same class of langauges (the regular langauges)  So regular languages can be defined as regular expressions.",
            "title": "Kleene's Theorem"
        },
        {
            "location": "/Inf2A - Formal Languages/#kleene-algebra",
            "text": "Regular expressions can be manipulated with kleene algebra without changing the language     ...",
            "title": "Kleene algebra"
        },
        {
            "location": "/Inf2A - Formal Languages/#ardens-rule",
            "text": "Given an equation of the form   its smallest solution is  . If  , this is the only solution",
            "title": "Ardens rule"
        },
        {
            "location": "/Inf2A - Formal Languages/#dfas-to-regular-expressions",
            "text": "For each state  , the variable   stands for the set of states that take us from   to the accepting state.",
            "title": "DFA's to regular expressions"
        },
        {
            "location": "/Inf2A - Formal Languages/#applications-of-regular-languages",
            "text": "",
            "title": "Applications of regular languages"
        },
        {
            "location": "/Inf2A - Formal Languages/#grep",
            "text": "Search for prices in pounds and pence:  grep \"[0-9]*\\.[0-9][0-9]\" document.txt   fgrep  searches for one or more fixed string, using an efficent string matching algorthum  grep  searches for strings matching a pattern  egrep  searches for strings matching an extended pattern",
            "title": "Grep"
        },
        {
            "location": "/Inf2A - Formal Languages/#string-searching",
            "text": "Supose we want to search for string   in document  . An efficent implentation is the Knuth-Morris-Pratt algorithum:   Convert NFA accepting       to DFA M (costs some time, worth it for short   and long  ).  Run   through   (each character in   processed once, no buffering).  Every time we get to the accepting state of  , signal a hit.   This can be extended to search for mulitple strings in parralel with a more complex NFA.",
            "title": "String searching"
        },
        {
            "location": "/Inf2A - Formal Languages/#pattern-searching",
            "text": "egrep  will print all lines containing a match.   We can convert a pattern into a (smallish) NFA.  We can run the NFA using the just-in-time simulation (avoids exponential state-space blow-up).",
            "title": "Pattern searching"
        },
        {
            "location": "/Inf2A - Formal Languages/#data-validation",
            "text": "Within XML we can enforce constraints on parts of the data.  For text fields on web froms, we can check if the input text is in the correct form.",
            "title": "Data validation"
        },
        {
            "location": "/Inf2A - Formal Languages/#lexing",
            "text": "Even tough a higher level language is to complex to be regular, regular expressions can be used to identify the fundemental building blocks (tokens) of the language.   Lexical Analysis  Breaking up source code into a series of tagged tokens (lexemes)  Lexical class  The class of a token, for example  1000  would be an integer literal,  foobar  would be an identifier.",
            "title": "Lexing"
        },
        {
            "location": "/Inf2A - Formal Languages/#how-lexers-work",
            "text": "Build NFA's for the lexical classes   (in order of priority)  Run the 'parrallel' automaton   until it expires  The last point in which we were in an accepting state is the largest match, chose the smallest   such that were in an accepting state of  . Chose class   as the lexical class for   which is the highest priorty.  Perform the specified action for the class",
            "title": "How lexers work"
        },
        {
            "location": "/Inf2A - Formal Languages/#verification",
            "text": "Regulare langauge theory can help verify desirable properites:   Saftey, i.e.  bad things dont happen  Liveness, i.e.  good things do happen  Fierness, i.e.  things good for some processes dont cause to much badness to others",
            "title": "Verification"
        },
        {
            "location": "/Inf2A - Formal Languages/#example",
            "text": "Suppose we have two processes   that have use of a shared resource, but must not be given access at the same time.   can comunicate using three shared flags\n-  req0  initially false, whether   wants access\n-  req1  initially false, whether   wants access\n-  turn  who is being allowed a turn  Code for   when it wants access  req0 = true\nturn = 1\nwhile(req1 && turn == 1) WAIT\n// P0 can now access resource\nreq0 = false  Code for   is the same with   swapped and  req0  and  req1 swapped.  This can be moddled by a finite state machine:  The language for the complete system can now be obtained: \nwhere   is the interleaving of regular langauges.  Now machine   with 200 states can be built.  Now we can verify\n- Mutual exclusion:   and   can never access simulaneously.\n- Progress",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Formal Languages/#the-pumping-lemma",
            "text": "",
            "title": "The Pumping Lemma"
        },
        {
            "location": "/Inf2A - Formal Languages/#loops-in-dfas",
            "text": "A machine   with states   will always have a loop since we can process strings   where  , thus we visit some state twice. Thus any string   can be decomposed into:\n-  , the prefix of   that leads to the first visit of the repeated state  \n-  , the loop which goes from   to  \n-  , whatever is left of   after",
            "title": "Loops in DFA's"
        },
        {
            "location": "/Inf2A - Formal Languages/#lemma",
            "text": "Suppose   is a regular language, then   has the following property  :  Their exists   such that, for all strings   with   and  , there exist strings   such that  ,  , and for every   we have",
            "title": "Lemma"
        },
        {
            "location": "/Inf2A - Formal Languages/#contrapositive",
            "text": "To prove a language is not reqular, we take the contrapositive of the pumping lemma  :  For all  , there exist strings   with   and   such that, for every decomposition of   as   where  , their is some   which",
            "title": "Contrapositive"
        },
        {
            "location": "/Inf2A - Formal Languages/#using-the-pumping-lemma",
            "text": "Your argument must work for all  .  Choose strings   (which might depend on  ) to satisfy   and  . Additionaly   should \"disallow pumping\" (number of loops depend on language).  Your argument must work for all decompositions of   as   with  .  Choose the number   such that  , here   might depend on all the previous data.",
            "title": "Using the pumping lemma"
        },
        {
            "location": "/Inf2A - Formal Languages/#example-1",
            "text": "Show that   is not regular   Suppose   Let  ,  ,   Let    for some   Thus   , thus   satisfys  , thus it is not regular",
            "title": "Example 1"
        },
        {
            "location": "/Inf2A - Formal Languages/#example-2",
            "text": "Show that   is not regular   Suppose   Let  ,  ,   so   Given   where  ,   (since   is non empry and the length of   is length  )  Let  , thus  ,   The length of the new string becomes   The next perfect string after   is  k^2 + 2k + 1 k^2 (k+1)^2 =   Thus   isnt a perfect square, thus   Thus   satisfys  , thus it is not regular",
            "title": "Example 2"
        },
        {
            "location": "/Inf2A - Formal Languages/#context-free-languages",
            "text": "",
            "title": "Context free languages"
        },
        {
            "location": "/Inf2A - Formal Languages/#context-free-grammers",
            "text": "This grammer generates simple arithmetic expressions such as   and  .   Terminals  The ultimate constituants of the phrases generated  Non-terminals  Designate varoious sub-phrases, such as  Exp ,  Num  etc  Productions  Defines how a non-terminal is expanded into more terminals/non-terminals  Start symbol  Non-terminal to start with, in this case  Exp  Syntax tree  Tree formed starting from the start symbol we grow syntax trees my expanding the non-terminals using the grammer rules.  Structural ambiguity  Strings that have multiple possible syntax trees   The language generated by a grammer is the set of  strings of terminals  that can be derived.",
            "title": "Context free grammers"
        },
        {
            "location": "/Inf2A - Formal Languages/#derivations",
            "text": "At each stage we choose a non terminal and expand using a suitable rule, when their are no terminals left a derivation is complete.    Leftmost derivation  Derivation expanding the leftmost terminal each time  Rightmost derivation  Derivation expanding the rightmost terminal each time",
            "title": "Derivations"
        },
        {
            "location": "/Inf2A - Formal Languages/#formal-defintion",
            "text": "A context free grammer (CFG)   consists of:   A finite set of   non terminals  A finitie set of   terminals, disjoint from   A finite set   of productions of the form  , where   A choise of start symbol",
            "title": "Formal defintion"
        },
        {
            "location": "/Inf2A - Formal Languages/#regular-languages-to-context-free",
            "text": "Input symbols becomes termials  States become non-terminals  Initial state becomes start state  Every transition   becomes   Accepting states   becomes    Consider the following FSM.",
            "title": "Regular languages to context free"
        },
        {
            "location": "/Inf2A - Formal Languages/#todo-insert-fsm",
            "text": "The context free form is",
            "title": "TODO: insert FSM"
        },
        {
            "location": "/Inf2A - Formal Languages/#pushdown-automata",
            "text": "Pushdown Automata  A control unit with finite states, equipped with a read head (that can only read the input left to right). Additionaly their is a stack which can be read, pushed and popped.   Consider a PDA with a single state  , input alphabet   and stack alphabet  . Call   the initial stack symbol.  The DA has four transitions in the form         For example, if the current read symbol is   and the current stack symobl is  , pop   then push   then      Transition  Input  Stack state           1      2      3      2      3      3      4       If the input string has well matched brackets the stack will be emptyed when the input string ends, otherwise the brackets are not well matched.",
            "title": "Pushdown Automata"
        },
        {
            "location": "/Inf2A - Formal Languages/#formal-defintiion",
            "text": "A finite set of control states   Fintite set of input alphabet   Finite stack alphabet   including start symbol   Start state   Finite transition relation    A string is accepted if their is some run of   on   starting at   with stack  , and finishing (at any control state) with an empty stack having consumed all of  .",
            "title": "Formal defintiion"
        },
        {
            "location": "/Inf2A - Formal Languages/#cggs-to-npdas",
            "text": "Stingle state  , input alphabet  , and stack alphabet  , and   as the initial stack symbol  For each production   in P, include an   transition,   For each terminal  , include a transition    Consider parsing the string   using the CFG earlier. At each stage we guess the correct rule to apply.   Note that each stage, the combination of input read and stack state gives a  sentential form . The result is the leftmost derivation.",
            "title": "CGG's to NPDA's"
        },
        {
            "location": "/Inf2A - Formal Languages/#ll1-pridictive-parsing",
            "text": "LL(1)  Read the input from the  L eft, build the  L eftmost derivation, looking  1  symbol ahead.   To determine what rules to apply (not guess as we have done before) we use two pieces of infomation:   Current lexical token  The nonterminal to be expanded   If it is always possible to determine the rule using these pieces of infomation the languages is said to be LL(1).",
            "title": "LL(1) Pridictive Parsing"
        },
        {
            "location": "/Inf2A - Formal Languages/#example_1",
            "text": "Consider the following CFG   We can derive a  parse table , which is a table of terminals and nonterminals that tells us what rule to apply for each case.   The   signifys the end of input. Blank entries corresponds to illegals states thus the string is illegal.",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Formal Languages/#parsing",
            "text": "Given a parse table we can parse a string using the following:   Begin with the start symbol   on the stack.  If the current input symbol   (could be  ), and the current stack symbol is a non-terminal  , look up the rule for   in the table.  If the current input symbol is   and the stack symbol is   just pop   and advance input.  Accept if the stack empties with   as the input symbol.",
            "title": "Parsing"
        },
        {
            "location": "/Inf2A - Formal Languages/#example_2",
            "text": "Conside the input string   and the parse table from above:   Parsing is as follows:",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Formal Languages/#formal-defintion_1",
            "text": "is LL(1) if for each terminal   and nonterminal  , there is some production   with the following property:  If   is a sentential form appearing in a leftmost derivation of some string  , the next sentential form appearing in the derivation is necessarily  .",
            "title": "Formal defintion"
        },
        {
            "location": "/Inf2A - Formal Languages/#non-ll1-grammers",
            "text": "Consider the folloing grammer:   Now suppose the initial stack is   and the first input symbol is  . We cant decide without looking ahead, if the rule to apply is   or  .",
            "title": "Non-LL(1) grammers"
        },
        {
            "location": "/Inf2A - Formal Languages/#generating-parse-tables",
            "text": "The set of all terminals that can appear at the start of a phrase derived from  .   The set of all terminals that can appear immediately after X in some sentential form derived from the start symbol  .  can be empty   can begin with   , and     For each nonterminal  , compute two sets called   and  .  First sets  Compute set  , which is the set of nonterminals that 'can be  '.  Add   to   whenever   is a production  .  If   is a production and all   are already in  , add   to  .  Repeat until   stabilizes    Set   for each  .  For each nonterminal  , initially set   to   if  , or   otherwise.  For each production   and each  , if   and  , add   to  .  Repeat step (d.) until all   sets stabalize     Follow sets  Intially set   for the start symbol  , and   for all nonterminals  .  For each production  , each splitting of   as   where   1, and each   with  , add all of   (excluding  ) to  .  For each production   and each splitting of   as   or   with  , add all of   to  .  Repeat step 3 until all   sets stabalize.      Use the First and Follow sets to fill out the parse tables.  For each production   of   in turn:  For each terminal  , if    can begin with   , insert   in row  , column  .  If    can be empty , then for each   (where b may be  ), insert   in row  , column  .  If their are clashes, the grammer is not LL(1).",
            "title": "Generating parse tables"
        },
        {
            "location": "/Inf2A - Formal Languages/#example_3",
            "text": "Consider the following CFG    First sets      (since  )   (since  )   (since  )   (since  )     Follow sets  Start with        For production     can be empty    Insert the production into   and     For production    can begin with   Insert the production into     For prodcution    can begin with   Insert the production into",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Formal Languages/#problems-with-grammers",
            "text": "They may be ambiguous (can have more than one leftmost derivation)  They may have shared prefixes  They may be left-recursive e.g.",
            "title": "Problems with grammers"
        },
        {
            "location": "/Inf2A - Formal Languages/#ambiguity",
            "text": "Here is an example of an ambigious statement:   For example,  , could be parsed as   or  .  To make it  unambiguous , we can add nonterminals, to capture distinct classes of expressions   Note this is note quite LL(1) yet.",
            "title": "Ambiguity"
        },
        {
            "location": "/Inf2A - Formal Languages/#shared-prefixs",
            "text": "In this example, an LL(1) parser would have no way to choose between these rules. The solution is to 'factor out' the common parts to delay the choice.",
            "title": "Shared Prefixs"
        },
        {
            "location": "/Inf2A - Formal Languages/#left-recursion",
            "text": "Conside the rules   Notice how it must start with  , and that it is followed by zero or more   or  . We can use this observation to remove the left recursion like so:",
            "title": "Left recursion"
        },
        {
            "location": "/Inf2A - Formal Languages/#chomsky-normal-form",
            "text": "A context free grammer   is in  Chomsky normal form (CNF)  if all productions are of the form:   If we disregard the empty string every language   can be transformed into a grammer   in CFG",
            "title": "Chomsky Normal Form"
        },
        {
            "location": "/Inf2A - Formal Languages/#converting-to-cfg",
            "text": "Remove all   productions, and for every rule   where   can be empty, add a new rule   Remove 'unit' productions  .  For each terminal, introduce a nonterminal  , for all rules   where  . Replace each   with   For every production   with  , add new symbols   and replace the rules with  .",
            "title": "Converting to CFG"
        },
        {
            "location": "/Inf2A - Formal Languages/#example_4",
            "text": "Consider the grammer    Remove   productions  Remove 'unit productions'    Add   productions  Split long productions",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Natural Languages/",
            "text": ".latex-box.math-false {text-align: center;}\n.math-true {vertical-align: middle;}\n\n\n\nInf2A - Natural Languages\n\n\nMorphology parsing\n\n\n\n\nMorphology\n\n\nThey study of the structure of words.\n\n\nStem\n\n\nPart of a word with no \naffixes\n\n\nAffix\n\n\nParts of a word added to the stem, including prefixes, suffixes, infixes and circumfixes\n\n\nInflection\n\n\nstem + grammer affix - word does not change grammatical class e.g. walk \n walking\n\n\nDerivation\n\n\nstem + grammar affix - word changes grammatical form e.g. computerize \n computerization\n\n\nComponding\n\n\nstem + stem e.g. doghouse\n\n\nCliticization\n\n\ne.g. I've, we're, he's\n\n\nLexical form\n\n\nstem + the properties of that word e.g. cats \n cat+N+PL, walking \n walk+V+PresPart, smoothest \n smooth+Adj+Sup\n\n\nMorphology parsing\n\n\nExtracting the lexical form from a a word\n\n\n\n\nWhy morphology parsing?\n\n\n\n\nPrerequsite of \ngrammatical parsing\n.\n\n\nSearch engines\n, a search for 'foxes' should also return documents including 'fox'.\n\n\nSpell checking\n, usings rules to check if the correct form of a word is used.\n\n\n\n\nFinite-state transducers\n\n\nFinite-state transducers are an extension of finite state machines which also produce and output.\n\n\n\n    \n\n        \n\n        \n\n        \n\n        \n\n        \n\n        \nb : \u03b5\n\n        \n\n        \n\n        \nb : \u03b5\n\n        \n\n        \na : 0\n\n        \n\n        \n\n        \na : 1\n\n        \n\n        \n\n        \n\n    \n\n\n\n\nFor example the string \naabaaabbab\n becomes \n001111\n\n\nFormal defintion\n\n\n\n\nAn input language \n and an output language \n\n\nSets \n as in an ordinary NFA\n\n\nA transition relation \n\n\n\n\nLexical to intermediate form\n\n\nWe want to convert a lexical form such as \nfox+N+PL\n into an intermediate form \nfox ^ s #\n, while taking into account irregualr forms such as goose/geese. We can do this with a transducer.\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \nirregular noun\n\n    \n\n    \n\n    \nregular noun\n\n    \n\n    \n\n    \nirregular noun\n\n    \n\n    \n\n    \n+PL : ^s#\n\n    \n\n    \n\n    \n+SG : #\n\n    \n\n    \n\n    \n+N : \u03b5\n\n    \n\n    \n\n    \n+N : \u03b5\n\n    \n\n    \n\n    \n+N : \u03b5\n\n    \n\n    \n\n    \n\n    \n\n    \n+PL : #\n\n    \n\n    \n\n    \n+SG : #\n\n\n\n\nFirst we copy the \nstem\n, then we map the labels to the correct intermediate output.\n\n\nIntermediate to surface form\n\n\nTo convert from intermediate form to surface form, we apply a number of \northographic rules\n such as:\n\n\n\n\nE-insertion:\n Insert \ne\n after \ns\n, \nz\n, \nx\n, \nch\n and \nsh\n before a word-final morpheme e.g. fox \n foxes.\n\n\nE-deletion:\n Delete \ne\n before a suffix beginning with \ne\n or \ni\n e.g. love \n loving.\n\n\nConstant doubling:\n Single consonants \nb\n, \ns\n, \ng\n, \nk\n, \nl\n, \nm\n, \nn\n, \np\n, \nr\n, \ns\n, \nt\n, \nv\n are doubled before suffix \ned\n or \ning\n e.g. beg \n begging \n\n\n\n\nParts-of-speech\n\n\nParts-of-speech is the process of tagging words in a sentence. Words are split into classes with the same distribution, such as:\n\n\n\n\nCrocodile\n\n\nPencil\n\n\nMistake\n\n\n\n\nThese (singular, countable) nouns can appear in the same grammatical contexts such as:\n\n\n\n\nJim saw the crocodile\n\n\nJim saw the penci\n\n\nJim saw the mistake\n\n\n\n\nClassifying words\n\n\nCriteria for classifying words:\n\n\n\n\nDistributional\n criteria - Where can the words occor?\n\n\nMorphological\n criteria - What form doest the word have? What affixes doest it have?\n\n\nNotinal\n criteria - What consept doest the word refer to, e.g. nouns refer to people, places and things.\n\n\n\n\nClasses of words are split into open and clossed:\n\n\n\n\nOpen classes\n are large, and fluid, and (often) stable under translation, e.g. nouns, verbs, adjectives adverbs.\n\n\nClosed classes\n small, fixed class which varys across languages.\n\n\n\n\nNouns\n\n\n\n\nNotionaly\n nouns generally refer to living things, places, non-living things or concepts.\n\n\nFormally\n \n-ness\n, \n-tion\n, \n-ity\n and \n-ance\n tend to indicate nouns.\n\n\nDistributionaly\n nouns can appear with posession e.g. \nhis car\n\n\n\n\nVerbs\n\n\n\n\nNotionaly\n verbs refer to actions.\n\n\nFormally\n \n-ate\n, \n-ize\n tend to be verbs, and \n-ing\n tends to be the present participle of a verb.\n\n\nDistributionaly\n verbs appear in diffrent contexts based on their type, for example base form verbs can appear as infinitives e.g. \nto jump\n.\n\n\n\n\nAdjectives\n\n\n\n\nNotionaly\n opinions about nouns.\n\n\nFormally\n \n-al\n, \n-ble\n and \n-ous\n tend to be adjectives.\n\n\nDistributionaly\n adjectives usually appear before a noun or after a form of \nbe\n.\n\n\n\n\nAdverbs\n\n\n\n\nNotionaly\n opinions about verbs.\n\n\nFormally\n \n-ly\n tend to be adverbs.\n\n\nDistributionaly\n adverbs tend to appear next to a verb, adjective or the start of a sentence.\n\n\n\n\nClose classes\n\n\n\n\nPropositions:\n on, under, over, near, by, at, from, to, with\n\n\nDeterminers:\n a, an, the\n\n\nConjunctions:\n and, but, or, as, if, when\n\n\nParticles:\n up, down, on, off, in, out, at, by\n\n\nNumerals:\n one, two, three, first, second, third\n\n\n\n\nZipf's law\n\n\nGiven some courpus the \nfrequeny\n of any word is inversely proporitonal to its \nrank\n. \n\n\nIn other words, their is a small number of very common words, and a large number of infrequent words.\n\n\nParts-of-speech tagging\n\n\nPOS ambiguity\n\n\nThe difficulty of POS tagging is tagging ambigoius word tokens. In the brown corpus, only 10.4\nERROR\n of \nword tokens\n.\n\n\n\n\nHe wants \nthat/DT\n hat.\n\n\nIt is obvious \nthat/CS\n he wants a hat.\n\n\nHe wants a hat \nthat/WPS\n fits.\n\n\n\n\nBigram tagging\n\n\nWith bigram tagging, for each word we build a table of tags that counts how many instances of a tag appears after another tag.\n\n\n\n\nThe problem with this model is that one error will cause problems for the rest of the sentence.\n\n\nHidden markow models\n\n\nThe idea is to model and agent, who generate the senntece by a semi-random process. Our aim is to compute the seequence of hidden states with the highest probability. \n\n\nFormal definition:\n\n\n\n\nA set \n of \nstates\n with \n as the start state. (The non-start staes correspond to parts-of-speech).\n\n\nA \ntranstion probability matrix\n \n where \n is the probability of jumping from \n to \n. For each \n we require \n.\n\n\nFor each non-start state \n and word type \n, an \nemission probability\n \n of outputting \n upon entry into \n. (Ideally for each \n, \n). \n\n\nAssume were given an \nobserved sequence\n \n of word token genered by the HMM.\n\n\n\n\nFor example the sentence \nEdinburgh has a very rich history.\n has the following probability for one particular sequence of tags:\n\n\n\n\nThe virterbi algorithum\n\n\nThe example was just 1 possible way of tagging the sentence, for a set of tags \n and a sentence length \n, we would need to check \n possible taggings.\n\n\nThe virterbi algorithum gives us a way of reducing the exponential problem using dynamic programming.\n\n\n\n\nCreate a table \n where \n ranges over the tags and \n ranges of the indices in the sentence.\n\n\nFor all \n and \n \n\n\n\n\nEssentially we find the probability of ending on the first word, then we can use this to define the probability of the second words (for each tag), and so on.\n\n\nExample\n\n\nGiven the sentence \ndeal talks fail\n and the possible tags \n (verb) and \n (noun), find the highest probable sequence of tags, given the following transition matrix:\n\n\n\n\nand emission matrix:\n\n\n\nWe get the following table:\n\n\n\n\nThe max of the last column is \nfail/N\n, \nfail/N\n came from \ntalks/V\n which came from \ndeal/N\n. Thus the highest probable sequence is \ndeal/N talks/V fail/N\n.\n\n\nParsing natrual language\n\n\n\n\nConstiuents\n\n\nGroups of words which stand on their own.\n\n\n\n\nFor example,\nKim [read a book], [gave it to Sandy], and [left].\n\n\nHead and Phrases\n\n\nThe head of the phrase is the most important word in the pharse, for example \nread\n in \nread a book\n.\n\n\nTheir are diffrent types of phrase:\n- Noun phrase\n- Adjective phrase\n- Propositional phrase\n- ...\n\n\nDiffrent languages have \nheads\n in diffrent places, for example Japanese is \nhead-final\n and Irish is \nhead-intial\n.\n\n\nDesirable properies of a grammer\n\n\n\n\nA \nfinitite\n specification of strings of the language (rather than a list of sentences).\n\n\n\n\nSould be \nreavealing\n, in allowing strings to be associated with meaning (semantics) in a systematic way.\n\n\n\n\n\n\nCFG's provide a good approximation\n\n\n\n\nSome NL features require context sensative\n\n\n\n\nGrammer for english\n\n\nGrammer rules\n\n\nS  -> NP VP\nNP -> Det N\nNP -> Det N PP\nNP -> Pro\nVP -> V NP PP\nVP -> V NP\nVP -> V\nPP -> Prep NP\n\n\n\n\nLexical rues\n\n\nDet -> a  | the | her\nN -> man | park | duck | telescope\nPro -> you\nV -> saw\nPrep -> \n\n\n\n\nStructual ambiguity\n\n\nParsing algorithums\n\n\n\n\nRecursive desent\n (top-down). Simple, but innefficent.\n\n\nShift-reduce\n (bottom up).\n\n\nCocke-Younger-Kasami algorithm\n (bottom up). Works for\nany CFG with reasonable efficiency.\n\n\nEarley algorithm\n (top down). Chart parsing enhanced\nwith prediction.\n\n\n\n\nRecursive desent\n\n\nShift-reduce\n\n\nCYK algorithm\n\n\nBy restructuring grammer to remove left-recursion we may make it less \nrevealing\n. For example:\n\n\n\n\nJohn\u2019s sister\n\n\nJohn\u2019s mother\u2019s sister\n\n\nJohn\u2019s mother\u2019s uncle\u2019s sister\n\n\nJohn\u2019s mother\u2019s uncle\u2019s sister\u2019s niece\n\n\n\n\nNumber of parse trees\n\n\nLet \n be the number of binary trees over a sentence of length \n. The root of this tree has two subtrees: one over \n words \n and one over \n. Hence for all \n we can combine any subtree over \n words with any subtree over \n words.\n\n\n\n\nAlgorithum\n\n\nGeneral algorithum:\n\n\nfunction CKY-Parse(words, grammar) returns table for\n    # Loop over the columns\n    j \u2190 from 1 to Length(words) do\n        # Fill the bottom cell\n        table[j \u2212 1, j] \u2190 {A | A \u2192 words[j] \u2208 grammar}\n        # Fill cells i, j\n        for i \u2190 from j \u2212 2 downto 0 do\n            for k \u2190 i + 1 to j \u2212 1 do\n                # Check the grammer for rules that link [i, k] with [k, j]. \n                # For every rule store it in [i, j].\n                table[i, j] \u2190 table[i, j] \u222a \n                    {A | A \u2192 BC \u2208 grammar, B \u2208 table[i, k], C \u2208 table[k, j]}\n\n\n\n\nSuccint representation of CKY\n\n\nIf we have a boolean \n such that \n is true if their is a subphrase that domminates i through j words.\n\n\n\n\nSeed for \n.\n\n\n is true if their is a rule \n where \n is the \n word in the string.\n\n\nDisadvantages\n\n\n\n\nGrammers not in CNF\n: The CYK algorithum can work with non CNF grammers by splitting spans into all possible subspans of the RHS. The problem with this is that complexity goes up steeply, \n where \n is the maxumum length of the RHS.\n\n\nUnnessisary constinuents\n: For the language of \n with grammer rules \n and \n, if we parse strings such as \ncaaaaa\n we generate both \n and \n constituents, however at the end only one of them is needed\n\n\n\n\nEarley Parsing\n\n\nThe key idea is as well as \ncompleted productions\n (ones whose entire RHS have been recognized), we also record \nincomplete productions\n (ones for which there may so far be only partial evidence).\n\n\nThe table entries are represented as \ndotted-rules\n\n\n\n\n [0, 0] - A \n is \npredicted\n at the start of the sentence\n\n\n [1, 2] - An \n is \nin progress\n seen \n, \n expected\n\n\n [0, 3] - A \n \nhas been found\n starting at 0 and ending at 3\n\n\n\n\nBasic algorithum\n\n\n\n\nPredict all the starts upfrount, working topdown from \n\n\nFor each word in the input\n\n\nScan in the word\n\n\nComplete or extend exsisting states based on matches\n\n\nAdd new predictions\n\n\n\n\n\n\nWhen out of words, look at the chart for a parse\n\n\n\n\nPrediction\n\n\nScanner\n\n\nCompleter\n\n\nExample\n\n\n\n\nProbablistic Context-Free Grammers\n\n\nWhy use probabilities in grammars:\n\n\n\n\nSyntatic disambiguation\n. Ambigiuity is unavoidable in natural language.\n\n\nCoverage\n. Handle production that we havent seen before.\n\n\nRepresentativeness\n. Handle diffrent types of texts.\n\n\n\n\nCYK and Earley algorithum returns all possible parses of a sentence however some of those parses may not make sence. By introducing probability we can discard the unlikey parses in order to find the best parse.\n\n\nFormal definition\n\n\nA PCFG \n is defined as follows:\n\n\n\n\n is the set of non-terminal symbols\n\n\n is the terminals (disjoint from \n)\n\n\n is a set of rules of the form \n  where \n and \n and \n is a number between 0 and 1.\n\n\n a start symbol \n\n\n\n\nA PCFG is a CFG with a probability \n attached to it.\n\n\nDisambiguation\n\n\nA PCFG assigns a probability to every parse tree, this is the product of all the rules that build it.\n\n\n\n\nTo find the most likey derivation we do the folllowing\n\n\n\n\nSince \n is positive and does not depend on \n we can ignore it.\n\n\nProbabilistic CYK\n\n\nInstead of a 2d table with a list of non-terminals that span \n through \n, we have a 3d table (with the 3rd dimension being the non-terminals) where each cell represents the probability of a non-terminal spanning \n through \n.\n\n\nCall \n the probability of the highest-probability derivation of \n from \n.\n\n\n\n\nFor each child we pick the best children mulitplied by the probabiltity to generate those children.\n\n\nExample\n\n\n\n\n\n\nParamerter estimation and Lexicalisation for PCFG's\n\n\nEstimate them from data\n\n\n\n\nNumber of times word \n occors with tag \n \n\n\nNumber of times tag \n appears after tag t' \n\n\n\n\nParameter estimation with treebank\n\n\n\n\nS1: [S [NP grass] [VP grows]]\nS2: [S [NP grass] [VP grows] [AP slowly]]\nS3: [S [NP grass] [VP grows] [AP fast]]\nS4: [S [NP bananas] [VP grow]]\n\n\n\n\n\n\nParameter estimation without treebank\n\n\nThe inside-outside algorithum:\n\n\n\n\nTake a CFG and set all rules to have equal probability.\n\n\nParse the corpus with the CFG.\n\n\nAdjust the probabilitys.\n\n\nRepreat steps 2-3 until probabilitys converge.\n\n\n\n\nIndependence problem\n\n\nIgnoring lexical infomation\n\n\nFor the sentences \nThe students dumped the sack in the bin\n and \nthe students spotted a flaw in the plan\n, \ndumped\n applys motion and is more likely to have a prepositional phrase than \nspotted\n.\n\n\nWe can lexicalize a PCFG by annotation non-terminals with \nhead words\n for example:\n\n\nVP(dumped) -> V(dumped) NP(sack) PP(in)\nVP(spotted) -> V(spotted) NP(flaw) PP(in)\nVP(dumped) -> V(dumpled) NP(sack)\n...\n\n\n\n\nHow many parameters are their\n\n\nIn CNF we will have rules:\n\n\nA[h] -> B[h] C[h']\n\n\n\n\nwhich has \n parameters where \n is the non terminals and \n is the vocabulary size\n\n\nAn improvement is to split rules into:\n\n\nA[h] -> B[h] C\nC -> C[h]\n\n\n\n\nwhich has \n, thus its linear with increase in vocabulary.\n\n\n?\n\n\nAgreement\n\n\n\n\nVerbs agree in person and number with their subjects\n\n\nTag questions agree in person, number, tence can mode with their main statement, and have the opposite polarity\n\n\nReflexcive pronouns follow suit in person, number and gender\n\n\n\n\nAgreement helps solve some ambiguity i.e \"the boy who eats flies docks\" -> \"the boys who eat fly ducks\"\n\n\nNode splitting via attributes\n\n\nOne solution is paramatizing the CFG productions such as \n\n\nS -> NP[p,n,nom] VP[p,n]\n\n\n\n\nHere we force the \nNP\n and \nVP\n to agree with each other in terms of person and tence.\n\n\nSemantics\n\n\n\n\nSemantics\n\n\nThe meaning of a sentence, finding the truth value within a world.\n\n\nDenotation\n\n\nLiteral meaning.\n\n\nCompositionality\n\n\nThe meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined.\n\n\nVerifiability\n\n\nOne must be able to use the meaning representation of a sentence to determine whether the sentence is true with respect to some given model of the world.\n\n\nUnambiguity\n\n\nA meaning representation shoub be unambigious, with one and only one interprentation.\n\n\nCanonical form\n\n\nThe meaning representation for sentence with the same meaning should (ideally) both be convertible into the same canonical form, that shows their equivence.\n\n\nLogical inference\n\n\nA good meaning representation should come with a set of rules for logical infernec eor deductions.\n\n\n\n\nPropositional logic\n\n\nA very simple system for meaning representation consisisting of \natomic sentence\n and \ncompound sentence\n which join atomic sentence with connectives.\n\n\n\n\nWe are unable to represent the internal structure of the proposition.\n\n\nUnable to express quantifiers.\n\n\n\n\nPredicate logic\n\n\nSentences are build up from:\n\n\n\n\nConstant and variable symbols\n\n\nEach constant symbol denotes one and only one entity.\n\n\nNot all entities have a constant that denotes them\n\n\nSeveral constant symbols may denote the same entity\n\n\n\n\n\n\nPredicate symbols that represents properies of entitys and relatiosn that hold between them\n\n\nEvery proedicate has a specific \narity\n\n\nDenotes propertys and relations\n\n\n\n\n\n\n\n\nCats are mammals\n => \n.\n\n\nI have a cat\n => \n.\nNote that \n is not equivalent, since if \n was not a cat, the statement is true.\n\n\nTypes\n\n\n\n\nUnary predicates \n<e, t>\n entitys to facts\n\n\nBinary preicates \n<e, <e,t>>\n\n\n<<e, t>, e>\n\n\n\n\nTo express these types, we use lambda expressions. e.g. \n which has the same truth value as \n.\n\n\nExample\n\n\n\n\nConside the question \"Who is the CEO of microsoft\"\n\n\nA pssible interpretation is \n\n\nWe may have a large database of CEO's in the form \n\n\nUsing the database we can test the truth value of these statements.\n\n\n\n\nSam loves kim\n will transform into:\n\n\n\n\nType raising\n\n\n\n\nWe are given \nSam\n and \nkim\n the semantic type e, and woamin the semantic type \n<e, t>\n\n\nSam\n => \n (type raising)\n\n\nevery woman\n => \n\n\n\n\nExample\n\n\nEvery student\n becomes",
            "title": "Inf2A   Natural Languages"
        },
        {
            "location": "/Inf2A - Natural Languages/#inf2a-natural-languages",
            "text": "",
            "title": "Inf2A - Natural Languages"
        },
        {
            "location": "/Inf2A - Natural Languages/#morphology-parsing",
            "text": "Morphology  They study of the structure of words.  Stem  Part of a word with no  affixes  Affix  Parts of a word added to the stem, including prefixes, suffixes, infixes and circumfixes  Inflection  stem + grammer affix - word does not change grammatical class e.g. walk   walking  Derivation  stem + grammar affix - word changes grammatical form e.g. computerize   computerization  Componding  stem + stem e.g. doghouse  Cliticization  e.g. I've, we're, he's  Lexical form  stem + the properties of that word e.g. cats   cat+N+PL, walking   walk+V+PresPart, smoothest   smooth+Adj+Sup  Morphology parsing  Extracting the lexical form from a a word",
            "title": "Morphology parsing"
        },
        {
            "location": "/Inf2A - Natural Languages/#why-morphology-parsing",
            "text": "Prerequsite of  grammatical parsing .  Search engines , a search for 'foxes' should also return documents including 'fox'.  Spell checking , usings rules to check if the correct form of a word is used.",
            "title": "Why morphology parsing?"
        },
        {
            "location": "/Inf2A - Natural Languages/#finite-state-transducers",
            "text": "Finite-state transducers are an extension of finite state machines which also produce and output.  \n     \n         \n         \n         \n         \n         \n         b : \u03b5 \n         \n         \n         b : \u03b5 \n         \n         a : 0 \n         \n         \n         a : 1 \n         \n         \n         \n       For example the string  aabaaabbab  becomes  001111",
            "title": "Finite-state transducers"
        },
        {
            "location": "/Inf2A - Natural Languages/#formal-defintion",
            "text": "An input language   and an output language   Sets   as in an ordinary NFA  A transition relation",
            "title": "Formal defintion"
        },
        {
            "location": "/Inf2A - Natural Languages/#lexical-to-intermediate-form",
            "text": "We want to convert a lexical form such as  fox+N+PL  into an intermediate form  fox ^ s # , while taking into account irregualr forms such as goose/geese. We can do this with a transducer.  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     irregular noun \n     \n     \n     regular noun \n     \n     \n     irregular noun \n     \n     \n     +PL : ^s# \n     \n     \n     +SG : # \n     \n     \n     +N : \u03b5 \n     \n     \n     +N : \u03b5 \n     \n     \n     +N : \u03b5 \n     \n     \n     \n     \n     +PL : # \n     \n     \n     +SG : #   First we copy the  stem , then we map the labels to the correct intermediate output.",
            "title": "Lexical to intermediate form"
        },
        {
            "location": "/Inf2A - Natural Languages/#intermediate-to-surface-form",
            "text": "To convert from intermediate form to surface form, we apply a number of  orthographic rules  such as:   E-insertion:  Insert  e  after  s ,  z ,  x ,  ch  and  sh  before a word-final morpheme e.g. fox   foxes.  E-deletion:  Delete  e  before a suffix beginning with  e  or  i  e.g. love   loving.  Constant doubling:  Single consonants  b ,  s ,  g ,  k ,  l ,  m ,  n ,  p ,  r ,  s ,  t ,  v  are doubled before suffix  ed  or  ing  e.g. beg   begging",
            "title": "Intermediate to surface form"
        },
        {
            "location": "/Inf2A - Natural Languages/#parts-of-speech",
            "text": "Parts-of-speech is the process of tagging words in a sentence. Words are split into classes with the same distribution, such as:   Crocodile  Pencil  Mistake   These (singular, countable) nouns can appear in the same grammatical contexts such as:   Jim saw the crocodile  Jim saw the penci  Jim saw the mistake",
            "title": "Parts-of-speech"
        },
        {
            "location": "/Inf2A - Natural Languages/#classifying-words",
            "text": "Criteria for classifying words:   Distributional  criteria - Where can the words occor?  Morphological  criteria - What form doest the word have? What affixes doest it have?  Notinal  criteria - What consept doest the word refer to, e.g. nouns refer to people, places and things.   Classes of words are split into open and clossed:   Open classes  are large, and fluid, and (often) stable under translation, e.g. nouns, verbs, adjectives adverbs.  Closed classes  small, fixed class which varys across languages.",
            "title": "Classifying words"
        },
        {
            "location": "/Inf2A - Natural Languages/#nouns",
            "text": "Notionaly  nouns generally refer to living things, places, non-living things or concepts.  Formally   -ness ,  -tion ,  -ity  and  -ance  tend to indicate nouns.  Distributionaly  nouns can appear with posession e.g.  his car",
            "title": "Nouns"
        },
        {
            "location": "/Inf2A - Natural Languages/#verbs",
            "text": "Notionaly  verbs refer to actions.  Formally   -ate ,  -ize  tend to be verbs, and  -ing  tends to be the present participle of a verb.  Distributionaly  verbs appear in diffrent contexts based on their type, for example base form verbs can appear as infinitives e.g.  to jump .",
            "title": "Verbs"
        },
        {
            "location": "/Inf2A - Natural Languages/#adjectives",
            "text": "Notionaly  opinions about nouns.  Formally   -al ,  -ble  and  -ous  tend to be adjectives.  Distributionaly  adjectives usually appear before a noun or after a form of  be .",
            "title": "Adjectives"
        },
        {
            "location": "/Inf2A - Natural Languages/#adverbs",
            "text": "Notionaly  opinions about verbs.  Formally   -ly  tend to be adverbs.  Distributionaly  adverbs tend to appear next to a verb, adjective or the start of a sentence.",
            "title": "Adverbs"
        },
        {
            "location": "/Inf2A - Natural Languages/#close-classes",
            "text": "Propositions:  on, under, over, near, by, at, from, to, with  Determiners:  a, an, the  Conjunctions:  and, but, or, as, if, when  Particles:  up, down, on, off, in, out, at, by  Numerals:  one, two, three, first, second, third",
            "title": "Close classes"
        },
        {
            "location": "/Inf2A - Natural Languages/#zipfs-law",
            "text": "Given some courpus the  frequeny  of any word is inversely proporitonal to its  rank .   In other words, their is a small number of very common words, and a large number of infrequent words.",
            "title": "Zipf's law"
        },
        {
            "location": "/Inf2A - Natural Languages/#parts-of-speech-tagging",
            "text": "",
            "title": "Parts-of-speech tagging"
        },
        {
            "location": "/Inf2A - Natural Languages/#pos-ambiguity",
            "text": "The difficulty of POS tagging is tagging ambigoius word tokens. In the brown corpus, only 10.4 ERROR  of  word tokens .   He wants  that/DT  hat.  It is obvious  that/CS  he wants a hat.  He wants a hat  that/WPS  fits.",
            "title": "POS ambiguity"
        },
        {
            "location": "/Inf2A - Natural Languages/#bigram-tagging",
            "text": "With bigram tagging, for each word we build a table of tags that counts how many instances of a tag appears after another tag.   The problem with this model is that one error will cause problems for the rest of the sentence.",
            "title": "Bigram tagging"
        },
        {
            "location": "/Inf2A - Natural Languages/#hidden-markow-models",
            "text": "The idea is to model and agent, who generate the senntece by a semi-random process. Our aim is to compute the seequence of hidden states with the highest probability.   Formal definition:   A set   of  states  with   as the start state. (The non-start staes correspond to parts-of-speech).  A  transtion probability matrix    where   is the probability of jumping from   to  . For each   we require  .  For each non-start state   and word type  , an  emission probability    of outputting   upon entry into  . (Ideally for each  ,  ).   Assume were given an  observed sequence    of word token genered by the HMM.   For example the sentence  Edinburgh has a very rich history.  has the following probability for one particular sequence of tags:",
            "title": "Hidden markow models"
        },
        {
            "location": "/Inf2A - Natural Languages/#the-virterbi-algorithum",
            "text": "The example was just 1 possible way of tagging the sentence, for a set of tags   and a sentence length  , we would need to check   possible taggings.  The virterbi algorithum gives us a way of reducing the exponential problem using dynamic programming.   Create a table   where   ranges over the tags and   ranges of the indices in the sentence.  For all   and      Essentially we find the probability of ending on the first word, then we can use this to define the probability of the second words (for each tag), and so on.",
            "title": "The virterbi algorithum"
        },
        {
            "location": "/Inf2A - Natural Languages/#example",
            "text": "Given the sentence  deal talks fail  and the possible tags   (verb) and   (noun), find the highest probable sequence of tags, given the following transition matrix:   and emission matrix:  We get the following table:   The max of the last column is  fail/N ,  fail/N  came from  talks/V  which came from  deal/N . Thus the highest probable sequence is  deal/N talks/V fail/N .",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Natural Languages/#parsing-natrual-language",
            "text": "Constiuents  Groups of words which stand on their own.   For example, Kim [read a book], [gave it to Sandy], and [left].",
            "title": "Parsing natrual language"
        },
        {
            "location": "/Inf2A - Natural Languages/#head-and-phrases",
            "text": "The head of the phrase is the most important word in the pharse, for example  read  in  read a book .  Their are diffrent types of phrase:\n- Noun phrase\n- Adjective phrase\n- Propositional phrase\n- ...  Diffrent languages have  heads  in diffrent places, for example Japanese is  head-final  and Irish is  head-intial .",
            "title": "Head and Phrases"
        },
        {
            "location": "/Inf2A - Natural Languages/#desirable-properies-of-a-grammer",
            "text": "A  finitite  specification of strings of the language (rather than a list of sentences).   Sould be  reavealing , in allowing strings to be associated with meaning (semantics) in a systematic way.    CFG's provide a good approximation   Some NL features require context sensative",
            "title": "Desirable properies of a grammer"
        },
        {
            "location": "/Inf2A - Natural Languages/#grammer-for-english",
            "text": "Grammer rules  S  -> NP VP\nNP -> Det N\nNP -> Det N PP\nNP -> Pro\nVP -> V NP PP\nVP -> V NP\nVP -> V\nPP -> Prep NP  Lexical rues  Det -> a  | the | her\nN -> man | park | duck | telescope\nPro -> you\nV -> saw\nPrep ->",
            "title": "Grammer for english"
        },
        {
            "location": "/Inf2A - Natural Languages/#structual-ambiguity",
            "text": "",
            "title": "Structual ambiguity"
        },
        {
            "location": "/Inf2A - Natural Languages/#parsing-algorithums",
            "text": "Recursive desent  (top-down). Simple, but innefficent.  Shift-reduce  (bottom up).  Cocke-Younger-Kasami algorithm  (bottom up). Works for\nany CFG with reasonable efficiency.  Earley algorithm  (top down). Chart parsing enhanced\nwith prediction.",
            "title": "Parsing algorithums"
        },
        {
            "location": "/Inf2A - Natural Languages/#recursive-desent",
            "text": "",
            "title": "Recursive desent"
        },
        {
            "location": "/Inf2A - Natural Languages/#shift-reduce",
            "text": "",
            "title": "Shift-reduce"
        },
        {
            "location": "/Inf2A - Natural Languages/#cyk-algorithm",
            "text": "By restructuring grammer to remove left-recursion we may make it less  revealing . For example:   John\u2019s sister  John\u2019s mother\u2019s sister  John\u2019s mother\u2019s uncle\u2019s sister  John\u2019s mother\u2019s uncle\u2019s sister\u2019s niece",
            "title": "CYK algorithm"
        },
        {
            "location": "/Inf2A - Natural Languages/#number-of-parse-trees",
            "text": "Let   be the number of binary trees over a sentence of length  . The root of this tree has two subtrees: one over   words   and one over  . Hence for all   we can combine any subtree over   words with any subtree over   words.",
            "title": "Number of parse trees"
        },
        {
            "location": "/Inf2A - Natural Languages/#algorithum",
            "text": "General algorithum:  function CKY-Parse(words, grammar) returns table for\n    # Loop over the columns\n    j \u2190 from 1 to Length(words) do\n        # Fill the bottom cell\n        table[j \u2212 1, j] \u2190 {A | A \u2192 words[j] \u2208 grammar}\n        # Fill cells i, j\n        for i \u2190 from j \u2212 2 downto 0 do\n            for k \u2190 i + 1 to j \u2212 1 do\n                # Check the grammer for rules that link [i, k] with [k, j]. \n                # For every rule store it in [i, j].\n                table[i, j] \u2190 table[i, j] \u222a \n                    {A | A \u2192 BC \u2208 grammar, B \u2208 table[i, k], C \u2208 table[k, j]}",
            "title": "Algorithum"
        },
        {
            "location": "/Inf2A - Natural Languages/#succint-representation-of-cky",
            "text": "If we have a boolean   such that   is true if their is a subphrase that domminates i through j words.   Seed for  .   is true if their is a rule   where   is the   word in the string.",
            "title": "Succint representation of CKY"
        },
        {
            "location": "/Inf2A - Natural Languages/#disadvantages",
            "text": "Grammers not in CNF : The CYK algorithum can work with non CNF grammers by splitting spans into all possible subspans of the RHS. The problem with this is that complexity goes up steeply,   where   is the maxumum length of the RHS.  Unnessisary constinuents : For the language of   with grammer rules   and  , if we parse strings such as  caaaaa  we generate both   and   constituents, however at the end only one of them is needed",
            "title": "Disadvantages"
        },
        {
            "location": "/Inf2A - Natural Languages/#earley-parsing",
            "text": "The key idea is as well as  completed productions  (ones whose entire RHS have been recognized), we also record  incomplete productions  (ones for which there may so far be only partial evidence).  The table entries are represented as  dotted-rules    [0, 0] - A   is  predicted  at the start of the sentence   [1, 2] - An   is  in progress  seen  ,   expected   [0, 3] - A    has been found  starting at 0 and ending at 3",
            "title": "Earley Parsing"
        },
        {
            "location": "/Inf2A - Natural Languages/#basic-algorithum",
            "text": "Predict all the starts upfrount, working topdown from   For each word in the input  Scan in the word  Complete or extend exsisting states based on matches  Add new predictions    When out of words, look at the chart for a parse",
            "title": "Basic algorithum"
        },
        {
            "location": "/Inf2A - Natural Languages/#prediction",
            "text": "",
            "title": "Prediction"
        },
        {
            "location": "/Inf2A - Natural Languages/#scanner",
            "text": "",
            "title": "Scanner"
        },
        {
            "location": "/Inf2A - Natural Languages/#completer",
            "text": "",
            "title": "Completer"
        },
        {
            "location": "/Inf2A - Natural Languages/#example_1",
            "text": "",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Natural Languages/#probablistic-context-free-grammers",
            "text": "Why use probabilities in grammars:   Syntatic disambiguation . Ambigiuity is unavoidable in natural language.  Coverage . Handle production that we havent seen before.  Representativeness . Handle diffrent types of texts.   CYK and Earley algorithum returns all possible parses of a sentence however some of those parses may not make sence. By introducing probability we can discard the unlikey parses in order to find the best parse.",
            "title": "Probablistic Context-Free Grammers"
        },
        {
            "location": "/Inf2A - Natural Languages/#formal-definition",
            "text": "A PCFG   is defined as follows:    is the set of non-terminal symbols   is the terminals (disjoint from  )   is a set of rules of the form    where   and   and   is a number between 0 and 1.   a start symbol    A PCFG is a CFG with a probability   attached to it.",
            "title": "Formal definition"
        },
        {
            "location": "/Inf2A - Natural Languages/#disambiguation",
            "text": "A PCFG assigns a probability to every parse tree, this is the product of all the rules that build it.   To find the most likey derivation we do the folllowing   Since   is positive and does not depend on   we can ignore it.",
            "title": "Disambiguation"
        },
        {
            "location": "/Inf2A - Natural Languages/#probabilistic-cyk",
            "text": "Instead of a 2d table with a list of non-terminals that span   through  , we have a 3d table (with the 3rd dimension being the non-terminals) where each cell represents the probability of a non-terminal spanning   through  .  Call   the probability of the highest-probability derivation of   from  .   For each child we pick the best children mulitplied by the probabiltity to generate those children.",
            "title": "Probabilistic CYK"
        },
        {
            "location": "/Inf2A - Natural Languages/#example_2",
            "text": "",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Natural Languages/#paramerter-estimation-and-lexicalisation-for-pcfgs",
            "text": "",
            "title": "Paramerter estimation and Lexicalisation for PCFG's"
        },
        {
            "location": "/Inf2A - Natural Languages/#estimate-them-from-data",
            "text": "Number of times word   occors with tag     Number of times tag   appears after tag t'",
            "title": "Estimate them from data"
        },
        {
            "location": "/Inf2A - Natural Languages/#parameter-estimation-with-treebank",
            "text": "S1: [S [NP grass] [VP grows]]\nS2: [S [NP grass] [VP grows] [AP slowly]]\nS3: [S [NP grass] [VP grows] [AP fast]]\nS4: [S [NP bananas] [VP grow]]",
            "title": "Parameter estimation with treebank"
        },
        {
            "location": "/Inf2A - Natural Languages/#parameter-estimation-without-treebank",
            "text": "The inside-outside algorithum:   Take a CFG and set all rules to have equal probability.  Parse the corpus with the CFG.  Adjust the probabilitys.  Repreat steps 2-3 until probabilitys converge.",
            "title": "Parameter estimation without treebank"
        },
        {
            "location": "/Inf2A - Natural Languages/#independence-problem",
            "text": "",
            "title": "Independence problem"
        },
        {
            "location": "/Inf2A - Natural Languages/#ignoring-lexical-infomation",
            "text": "For the sentences  The students dumped the sack in the bin  and  the students spotted a flaw in the plan ,  dumped  applys motion and is more likely to have a prepositional phrase than  spotted .  We can lexicalize a PCFG by annotation non-terminals with  head words  for example:  VP(dumped) -> V(dumped) NP(sack) PP(in)\nVP(spotted) -> V(spotted) NP(flaw) PP(in)\nVP(dumped) -> V(dumpled) NP(sack)\n...",
            "title": "Ignoring lexical infomation"
        },
        {
            "location": "/Inf2A - Natural Languages/#how-many-parameters-are-their",
            "text": "In CNF we will have rules:  A[h] -> B[h] C[h']  which has   parameters where   is the non terminals and   is the vocabulary size  An improvement is to split rules into:  A[h] -> B[h] C\nC -> C[h]  which has  , thus its linear with increase in vocabulary.",
            "title": "How many parameters are their"
        },
        {
            "location": "/Inf2A - Natural Languages/#_1",
            "text": "",
            "title": "?"
        },
        {
            "location": "/Inf2A - Natural Languages/#agreement",
            "text": "Verbs agree in person and number with their subjects  Tag questions agree in person, number, tence can mode with their main statement, and have the opposite polarity  Reflexcive pronouns follow suit in person, number and gender   Agreement helps solve some ambiguity i.e \"the boy who eats flies docks\" -> \"the boys who eat fly ducks\"",
            "title": "Agreement"
        },
        {
            "location": "/Inf2A - Natural Languages/#node-splitting-via-attributes",
            "text": "One solution is paramatizing the CFG productions such as   S -> NP[p,n,nom] VP[p,n]  Here we force the  NP  and  VP  to agree with each other in terms of person and tence.",
            "title": "Node splitting via attributes"
        },
        {
            "location": "/Inf2A - Natural Languages/#semantics",
            "text": "Semantics  The meaning of a sentence, finding the truth value within a world.  Denotation  Literal meaning.  Compositionality  The meaning of a complex expression is a function of the meaning of its parts and of the rules by which they are combined.  Verifiability  One must be able to use the meaning representation of a sentence to determine whether the sentence is true with respect to some given model of the world.  Unambiguity  A meaning representation shoub be unambigious, with one and only one interprentation.  Canonical form  The meaning representation for sentence with the same meaning should (ideally) both be convertible into the same canonical form, that shows their equivence.  Logical inference  A good meaning representation should come with a set of rules for logical infernec eor deductions.",
            "title": "Semantics"
        },
        {
            "location": "/Inf2A - Natural Languages/#propositional-logic",
            "text": "A very simple system for meaning representation consisisting of  atomic sentence  and  compound sentence  which join atomic sentence with connectives.   We are unable to represent the internal structure of the proposition.  Unable to express quantifiers.",
            "title": "Propositional logic"
        },
        {
            "location": "/Inf2A - Natural Languages/#predicate-logic",
            "text": "Sentences are build up from:   Constant and variable symbols  Each constant symbol denotes one and only one entity.  Not all entities have a constant that denotes them  Several constant symbols may denote the same entity    Predicate symbols that represents properies of entitys and relatiosn that hold between them  Every proedicate has a specific  arity  Denotes propertys and relations     Cats are mammals  =>  .  I have a cat  =>  .\nNote that   is not equivalent, since if   was not a cat, the statement is true.",
            "title": "Predicate logic"
        },
        {
            "location": "/Inf2A - Natural Languages/#types",
            "text": "Unary predicates  <e, t>  entitys to facts  Binary preicates  <e, <e,t>>  <<e, t>, e>   To express these types, we use lambda expressions. e.g.   which has the same truth value as  .",
            "title": "Types"
        },
        {
            "location": "/Inf2A - Natural Languages/#example_3",
            "text": "Conside the question \"Who is the CEO of microsoft\"  A pssible interpretation is   We may have a large database of CEO's in the form   Using the database we can test the truth value of these statements.   Sam loves kim  will transform into:",
            "title": "Example"
        },
        {
            "location": "/Inf2A - Natural Languages/#type-raising",
            "text": "We are given  Sam  and  kim  the semantic type e, and woamin the semantic type  <e, t>  Sam  =>   (type raising)  every woman  =>",
            "title": "Type raising"
        },
        {
            "location": "/Inf2A - Natural Languages/#example_4",
            "text": "Every student  becomes",
            "title": "Example"
        },
        {
            "location": "/Introduction-to-Computer-Systems/",
            "text": ".latex-box.math-false {text-align: center;}\n.math-true {vertical-align: middle;}\n\n\n\nFloating point\n\n\n\n\nMantissa/significand\n\n\nFractional part\n\n\nExponent\n\n\nPower of \n\n\n\n\n\n\nConvert \n in floating point form\n\n\n\n\n\n\n\n\nWhy normalize?\n\n\n\n\nSimplifiys machine representation\n\n\nSimplifys comparisons e.g. $0.00000101$ vs $0.000001$\n\n\nMore compact for small/large numbers\n\n\n\n\nIEEE 654 Floating Point Standard\n\n\n\n\n1'st bit is sign bit (\n)\n\n\n8 bits for exponent (\n)\n\n\nrest for mantissa (\n)\n\n\n\n\nEncoding:\n\n\n\n(127 is the \nbias\n)\n\n\n\n\nWhy bias?\n\n\n\n\nExponent can always be positive (no need to store sign bit)\n\n\nSimplifys comparison operations\n\n\n\n\nSpecial values\n\n\n\n\n\n\n\n\nExponenent\n\n\nMantisa\n\n\nNumber\n\n\n\n\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n1-254\n\n\nAnything\n\n\nFloating point number\n\n\n\n\n\n\n255\n\n\n0\n\n\nInfinity (signed)\n\n\n\n\n\n\n255\n\n\nnon-zero\n\n\nNaN (not a number)\n\n\n\n\n\n\n\n\nMIPS\n\n\n\n\nISA\n\n\nInstruction set architecture - The interface between the hardware and the software\n\n\nMIPS\n\n\nA real world ISA used by many diffrent processors since the 80s\n\n\n\n\nInstruction set\n\n\n\n\nAssemble Language\n\n\nSymbolic representation of machine instructions.\n\n\n\n\nMachine code are instructions stored as binary values, assembly language is a one-to-one mapping that allow human programmers to reason about programs.\n\n\nHigh level:\n\n\na[0] = b[0] + 10\n\n\n\n\nMIPS:\n\n\nlw r4, 0(r2)    # Load word - Get the value fo b[0] from memory and store in r4\nadd r5,r4,10    # Add - Compute b[0]+10 and store in r5\nsw r5,0(r1)     # Save word - Store r5 into a[0]\n\n\n\n\nMIPS does not allow accessing and operating on data at the same time\n\n\nArithmatic and Logical Operations\n\n\n\n\nadd a, b, c\n \n\n\nsub a, b, c\n \n\n\nsll a, b, c\n \n\n\nsrl a, b, c\n \n (logical)\n\n\nsra a, b, c\n \n (arithmatic)\n\n\n\n\nRegisters\n\n\n\n\nRegister\n\n\nStorage locations inside the processor that holds program variables and control state\n\n\n\n\n\n\nSome registers are special purpose\n\n\nRegister \n$zero\n is always zero for example\n\n\n$pc\n is the program counter, the adress of the next instruction\n\n\n$ra\n stores the return adress to return to after executing a method\n\n\n\n\n\n\nMost are for general use\n\n\n$s0-$s7\n are variables\n\n\n$t0-$t9\n are tempory variables\n\n\n\n\n\n\n\n\nEndinness\n\n\n\n\nEndinness\n\n\nThe sequential order in which bytes are arranged in memory\n\n\nBig-endian\n\n\nBytes orders by most significant bit\n\n\nLittle-endian\n\n\nBytes ordered by least significant bit\n\n\n\n\nMIPS instruction format\n\n\nEach assembly instruction translates into 1 machine instruction. Their are 3 formts of instructions\n\n\n\n\n\n\nR-format (e.g. \nadd\n, \nsub\n, \nand\n, \nor\n ...) \n\n\n\n\n\n\nI-format (e.g. \naddi\n, \nlw\n, \nsw\n, ...) \n\n\n\n\n\n\nJ-format (\nj\n) \n\n\n\n\n\n\nMIPS examples\n\n\nSwap\n\n\nThis function swaps two consecutive array elements \n\n\n# Compute the adress of the array\nsll $t0, $a1 2      # reg $t0 = idx * 4\nadd $t0, $a0, $t0   # reg $t0 = v + (idx * 4)\n                    # $t0 holds the address of array[idx]\n\n# Load the two values to be swapped\nlw $t1, 0($t0)      # reg $t0 = array[idx]\nlw $t1, 4($t0)      # reg $t0 = array[idx + 1]\n\n# Store the swapped values back in memory\nsw $t2, 0($t0)      # array[idx] = $t2\nsw $t1, 4($t0)      # array[idx+1] = $t0\n\n\n\n\nIf\n\n\nbeq $s1,$s2,label2      # if(s1 == s2) jump to label2\nstmnt1                  # else\nj label3 # skip stmnt2  # continue from label3\nlabel2: stmnt2          # body of if\nlabel3: stmnt3          # continue from if\n\n\n\n\nLoop\n\n\nloop:\n    beq $s1,$zero,end               # $s1 holds count\n    ...                             # Body of loop                           \n    j loop                          # Jump back to start\nend:\n    ...\n\n\n\n\nMethod calls\n\n\nTo jump into a method use \njal label\n (jump and link) which:\n\n\n\n\nSets \n$ra\n to \n$pc + 4\n (the next instruction)\n\n\nSets \n$pc\n to the adress of the label\n\n\n\n\nWhen returning user \njr $ra\n (jump register) which:\n\n\n\n\nSets \n$pc\n to the value of \n$ra\n, the adress we want to return to\n\n\n\n\nConvention\n\n\nIt is convention to use registers in a certain way\n\n\n\n\n$a0\n - \n$a4\n as method parameters\n\n\n$r1\n and \n$r2\n as return values\n\n\n$s0\n - \n$s7\n are preserved across call boundrys\n\n\n$t0\n - \n$t9\n are \nnot\n preserved across call boundrys\n\n\n\n\nNested calls\n\n\nIf we have a nested call, we can store the return adress onto the stack allowing us to nest \"infinitly\" (or until the machine runs out of stack space)\n\n\nTo push a word:\n\n\n\n\nMove the stack pointer down with \naddi $sp, $sp, -4\n\n\nSave the return adress onto the stack \nsw $ra 0($sp)\n\n\n\n\nTo pop a word:\n\n\n\n\nFetch return adress from the stack \nlw $ra, 0($sp)\n\n\nMove stack pointer up with \naddi $sp, $sp 4\n\n\n\n\nThe stack is also used to:\n\n\n\n\nSave \n$s\n registers\n\n\nPass and return values if their isnt enough registers\n\n\nLocal variables inside a function (that dont fit into registers)\n\n\n\n\nLogic design\n\n\n\n\nCombinational logic\n\n\nOut depends on the current input (no memory)\n\n\nSequential logic\n\n\nOutput depends of current input and some previous inputs (requires memory)\n\n\n\n\nCombinational logic circuits\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nInverter / NOT gate\n\n\nInverts the input signal\n\n\n\n\n\n\nOR gate\n\n\nOutputs 1, if atleast one input is 1\n\n\n\n\n\n\nAND gate\n\n\nOutputs 1, if both inputs \nare\n 1\n\n\n\n\n\n\nNAND gate\n\n\nOutputs 1, if bot inputs \nare not\n 1\n\n\n\n\n\n\n\n\n\n\nFunctional completeness\n\n\nSet of gates that can express \nany\n boolean function.\n\n\n\n\nFunctional-complete sets:\n\n\n\n\nAND + OR + NOT\n\n\nNAND\n\n\nNOR\n\n\n\n\nMultiplexer\n\n\n\n\nMultiplexer\n\n\nSelects one of multiple inputs\n\n\n\n\n\n\nTo convert this curcuit to logical gates, we look at its truth table\n\n\n\n\nNext we look at all the cases where the output is \n. And all the inputs and or each of these cases to get the following:\n\n\n\n\nThis is the sum of products form.\n\n\nArithmetic\n\n\nIf we used sum of products form for an adder (adds two binary numbers together), their would be way to many curcuits to reason about with 32/64 inputs. Instead we sequence a smaller curcuit.\n\n\nFull / 1-bit adder takes in 2 digits and a carry, produces result digit and carry out. We then use multiple copys to create a 32-bit adder (ripple carry adder):\n\n\n\n\nPropagation delay\n\n\n\n\nPropagation delay\n\n\nTime delay between input signal change and output signal change.\n\n\n\n\nThe propagation delay depends on:\n\n\n\n\nTechnology (transistor parameters, wire capacitance, etc).\n\n\nDelay through gate\n\n\nNumber of gates\n\n\n\n\nSequential logic\n\n\n\n\nSequential logic\n\n\nOutput depends on current and past input(s), i.e. the curcuit has memory.\n\n\n\n\nSR Latch\n\n\nThe SR latch is the simplest form of memory.\n\n\n\n\nIt has the following truth table\n\n\n\n\nClock\n\n\nWe know the because of propagation delay, curcuits with lots of gates take some time to output. Outputs may also fluctute if certain paths are shorted than others. The clock forces all state changes to happen when its 1, and guarentees the state is correct while it is 0, thus we avoid any occilation effecting calculations.\n\n\nLevel-triggered D Latch\n\n\nA latch which only only changes when the clock is 1.\n\n\n\n\nFor example \n changes before \n, but \n wont change until after \n.\n\n\nEdge-triggered D flip-flop\n\n\nWe can do better. Half a clock cycle is still alot of time, thus an edge-triggered D flip-flop will only change when the input changes on a \npositive clock edge\n.\n\n\n\n\nFor example \n doesnt cause a change since it happens after the clock edge.\n\n\nGeneral sequential logic curcuit\n\n\nNow if we tie multiple D flip-flops together (with a common clock) we get a register. Combine this with some combinational logic (whos state is saved by the registers to be used next cycle) and we arrive at a general sequential logic circuit.\n\n\nHardware FSM\n\n\nWe can use FSM to derive the seqential logic curcuit.\n\n\nProcessor Design - Single Cycle\n\n\n\n\nDatapath\n\n\nPerforms the data operations as controlled by the instructions\n\n\nControl\n\n\nControls the datapath, memory and I/O as controlled by the instructions\n\n\n\n\nMain functions\n\n\n\n\nFetch\n instructions from instruction memory\n\n\nRead the register operands\n\n\nUse the ALU for computation\n\n\nArithmetic\n\n\nMemory adress\n\n\nBranch target\n\n\n\n\n\n\nAccess data memory for load/store\n\n\nStore the result of the computation/data into the destination register\n\n\nUpdate the program counter (PC)\n\n\n\n\nR-Format instructions\n\n\n\n\nRead two register operands\n\n\nPerform the arthemetic/logical operation\n\n\nWrite register result\n\n\n\n\nLoad/Store instructions\n\n\n\n\nRead register operands\n\n\nCalculare adress using 16-bit offset\n\n\nUse ALU with sign-extend\n\n\n\n\n\n\nRead (for load) or write (for store) the memory.\n\n\n(Load only) Update destination register\n\n\n\n\nBranch instructions\n\n\n\n\nRead register operands\n\n\nCompare operands\n\n\nUse ALC, subtact and check Zero output\n\n\n\n\n\n\nCalculate target adress\n\n\nSign-extend the immediate\n\n\nShift left 2 places (word align)\n\n\nAdd to PC + 4\n\n\n\n\n\n\n\n\nFull Datapath\n\n\n\n\nControl part\n\n\nThe control component produces the signals that control the datapath by looking at the 6 bit op code.    \n\n\n\n\nALU control\n\n\nALU operations:\n\n\n\n\n\n\n\n\nType\n\n\nOperation\n\n\n\n\n\n\n\n\n\n\nData transfers\n\n\nAdd\n\n\n\n\n\n\nBranches\n\n\nSub\n\n\n\n\n\n\nOther\n\n\nLook at func field\n\n\n\n\n\n\n\n\nOther operations:\n\n\n\n\n0000\n - AND\n\n\n0001\n - OR\n\n\n0010\n - add\n\n\n0110\n - subtract\n\n\n0111\n - set-on-less than\n\n\n\n\nProcessor Design - Multicycle processer\n\n\nMotivations for multicycle processors:\n\n\n\n\nSpeed:\n In a single cycle processor, the clock cycle must be long enough for the most complex instruction.\n\n\nCost:\n Functional units (adders e.g.) cannot be re-used during instruction execution.\n\n\n\n\nBasic idea:\n\n\n\n\nBreak up execution of instructions to multiple cycles\n\n\nEnsure actions performed in a cycle are generic (common to many instructions)\n\n\nReuse datapath and control path components\n\n\n\n\nBuilding blocks\n\n\n\n\nOne memory\n\n\nShared between instructions and data\n\n\nCommon interface given an indress and data to write, produces the read data.\n\n\n\n\n\n\nRegisters\n\n\nRead early in instruction cycle\n\n\nWritten late\n\n\nNo chance of read/write contention\n\n\n\n\n\n\nOne ALU\n\n\nAll PC calculations\n\n\nAll arithmetic\n\n\n\n\n\n\n\n\nExample execution\n\n\n<=\n means the register \"get the value of\" at the end of the clock cycle.\n\n\nFetch\n\n\n\n\n\n\n\n\nIR <= Mem[PC]1\nPC <= PC + 4\n\n\n\n\n\n\nRead registers\n\n\n\n\nA <= Reg[IR[25:21]] // adress the register denoted by the instruction\nB <= Reg[IR[20:16]]\nALUOut <= PC + sgnext(IR[15:0] << 0) // compute branch offset\n\n\n\n\n\n\n\n\n\n\nR-type arithmetic\n\n\n\n\n\n\n\n\nALUOut <= A op B\n\n\n\n\n\n\nImediate arithmetic\n\n\n\n\nALUOut <= A + sgnext(IR[15:0])\n\n\n\n\n\n\nBranch completion\n\n\n\n\nif(A == B) PC <= ALUOut\n\n\n\n\n\n\nJump completion\n\n\n\n\nPC <= {PC[31:28], IR[25:0], 2'b00} // 4 most significant bits + jump adress + 00\n\n\n\n\n\n\nPerform store\n\n\n\n\nData and control path\n\n\n\n\n\n\nNew instruction register to store currently executing instruction\n\n\nA & B read register\n\n\nALU mux to control ALU inputs and operations\n\n\nALU out register\n\n\nMemory data register (stores data from memory)\n\n\n\n\nMemory\n\n\n\n\nTemporal locality\n\n\nRecently accessed memeory location is likely to be accessed again in the near future.\n\n\nSpatial locatility\n\n\nMemory location close to recently accessed location are likely to be accessed in the near future.\n\n\n\n\nMemory hierachy\n\n\n\n\n\n\nTemporal locality\n\n\n\n\nIf access data from slower memeory, move to faster memory\n\n\nIf data is faster memeory unused recently, move it to slower memory\n\n\n\n\n\n\n\n\nSpartial locality\n\n\n\n\nIf need to move a word from slower to faster memory, move adjacten words\n\n\nGives rise to blocks and pages, units of storage for contiguous memory.\n\n\n\n\n\n\n\n\nTerminology\n\n\n\n\nBlock (or line)\n\n\nUnit of data stored in the cache\n\n\nHit\n\n\nData is found in the cache\n\n\nMiss\n\n\nData not found\n\n\nHit rate\n\n\nFraction of accesses that are hits at any given level of hierarchy\n\n\nHit time\n\n\nTime required to access a level of the hierarchy, including time to determin wherer acess hit or miss\n\n\nMiss penalty\n\n\nExtra time required to fetch ablock in the next level down\n\n\n\n\nCache\n\n\n\n\nTag\n\n\nA word indicating the address of the main memroy block it holds (a subsection of the adress)\n\n\nValid bit\n\n\nIndicates the block is in use\n\n\n\n\nFully-associative cache\n\n\nThe tag is all of the bits an the adress except from the block offset. The byte offset is comprised of the lowest bits that denote the requested byte with in the cache block.\n\n\nCache replacement\n\n\n\n\nLeast recently used (LRU)\n\n\nEvict the cache block that hasnt been accessed in the longest time\n\n\nFirst in first out (FIFO)\n\n\nReplace in the same order as filled\n\n\n\n\nDirect-mapped cache\n\n\nA data item can be stored in one location only in the cache. The adress is split into the tag, index and byte offset. The index tells us where the memory is found/stored. If the tag in that position is the same, then their is a hit, otherwise its a miss.",
            "title": "Introduction to Computer Systems"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#floating-point",
            "text": "Mantissa/significand  Fractional part  Exponent  Power of     Convert   in floating point form",
            "title": "Floating point"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#why-normalize",
            "text": "Simplifiys machine representation  Simplifys comparisons e.g. $0.00000101$ vs $0.000001$  More compact for small/large numbers",
            "title": "Why normalize?"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#ieee-654-floating-point-standard",
            "text": "1'st bit is sign bit ( )  8 bits for exponent ( )  rest for mantissa ( )   Encoding:  (127 is the  bias )",
            "title": "IEEE 654 Floating Point Standard"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#why-bias",
            "text": "Exponent can always be positive (no need to store sign bit)  Simplifys comparison operations",
            "title": "Why bias?"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#special-values",
            "text": "Exponenent  Mantisa  Number      0  0  0    1-254  Anything  Floating point number    255  0  Infinity (signed)    255  non-zero  NaN (not a number)",
            "title": "Special values"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#mips",
            "text": "ISA  Instruction set architecture - The interface between the hardware and the software  MIPS  A real world ISA used by many diffrent processors since the 80s",
            "title": "MIPS"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#instruction-set",
            "text": "Assemble Language  Symbolic representation of machine instructions.   Machine code are instructions stored as binary values, assembly language is a one-to-one mapping that allow human programmers to reason about programs.  High level:  a[0] = b[0] + 10  MIPS:  lw r4, 0(r2)    # Load word - Get the value fo b[0] from memory and store in r4\nadd r5,r4,10    # Add - Compute b[0]+10 and store in r5\nsw r5,0(r1)     # Save word - Store r5 into a[0]  MIPS does not allow accessing and operating on data at the same time",
            "title": "Instruction set"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#arithmatic-and-logical-operations",
            "text": "add a, b, c    sub a, b, c    sll a, b, c    srl a, b, c    (logical)  sra a, b, c    (arithmatic)",
            "title": "Arithmatic and Logical Operations"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#registers",
            "text": "Register  Storage locations inside the processor that holds program variables and control state    Some registers are special purpose  Register  $zero  is always zero for example  $pc  is the program counter, the adress of the next instruction  $ra  stores the return adress to return to after executing a method    Most are for general use  $s0-$s7  are variables  $t0-$t9  are tempory variables",
            "title": "Registers"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#endinness",
            "text": "Endinness  The sequential order in which bytes are arranged in memory  Big-endian  Bytes orders by most significant bit  Little-endian  Bytes ordered by least significant bit",
            "title": "Endinness"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#mips-instruction-format",
            "text": "Each assembly instruction translates into 1 machine instruction. Their are 3 formts of instructions    R-format (e.g.  add ,  sub ,  and ,  or  ...)     I-format (e.g.  addi ,  lw ,  sw , ...)     J-format ( j )",
            "title": "MIPS instruction format"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#mips-examples",
            "text": "",
            "title": "MIPS examples"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#swap",
            "text": "This function swaps two consecutive array elements   # Compute the adress of the array\nsll $t0, $a1 2      # reg $t0 = idx * 4\nadd $t0, $a0, $t0   # reg $t0 = v + (idx * 4)\n                    # $t0 holds the address of array[idx]\n\n# Load the two values to be swapped\nlw $t1, 0($t0)      # reg $t0 = array[idx]\nlw $t1, 4($t0)      # reg $t0 = array[idx + 1]\n\n# Store the swapped values back in memory\nsw $t2, 0($t0)      # array[idx] = $t2\nsw $t1, 4($t0)      # array[idx+1] = $t0",
            "title": "Swap"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#if",
            "text": "beq $s1,$s2,label2      # if(s1 == s2) jump to label2\nstmnt1                  # else\nj label3 # skip stmnt2  # continue from label3\nlabel2: stmnt2          # body of if\nlabel3: stmnt3          # continue from if",
            "title": "If"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#loop",
            "text": "loop:\n    beq $s1,$zero,end               # $s1 holds count\n    ...                             # Body of loop                           \n    j loop                          # Jump back to start\nend:\n    ...",
            "title": "Loop"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#method-calls",
            "text": "To jump into a method use  jal label  (jump and link) which:   Sets  $ra  to  $pc + 4  (the next instruction)  Sets  $pc  to the adress of the label   When returning user  jr $ra  (jump register) which:   Sets  $pc  to the value of  $ra , the adress we want to return to",
            "title": "Method calls"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#convention",
            "text": "It is convention to use registers in a certain way   $a0  -  $a4  as method parameters  $r1  and  $r2  as return values  $s0  -  $s7  are preserved across call boundrys  $t0  -  $t9  are  not  preserved across call boundrys",
            "title": "Convention"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#nested-calls",
            "text": "If we have a nested call, we can store the return adress onto the stack allowing us to nest \"infinitly\" (or until the machine runs out of stack space)  To push a word:   Move the stack pointer down with  addi $sp, $sp, -4  Save the return adress onto the stack  sw $ra 0($sp)   To pop a word:   Fetch return adress from the stack  lw $ra, 0($sp)  Move stack pointer up with  addi $sp, $sp 4   The stack is also used to:   Save  $s  registers  Pass and return values if their isnt enough registers  Local variables inside a function (that dont fit into registers)",
            "title": "Nested calls"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#logic-design",
            "text": "Combinational logic  Out depends on the current input (no memory)  Sequential logic  Output depends of current input and some previous inputs (requires memory)",
            "title": "Logic design"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#combinational-logic-circuits",
            "text": "Name  Description      Inverter / NOT gate  Inverts the input signal    OR gate  Outputs 1, if atleast one input is 1    AND gate  Outputs 1, if both inputs  are  1    NAND gate  Outputs 1, if bot inputs  are not  1      Functional completeness  Set of gates that can express  any  boolean function.   Functional-complete sets:   AND + OR + NOT  NAND  NOR",
            "title": "Combinational logic circuits"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#multiplexer",
            "text": "Multiplexer  Selects one of multiple inputs    To convert this curcuit to logical gates, we look at its truth table   Next we look at all the cases where the output is  . And all the inputs and or each of these cases to get the following:   This is the sum of products form.",
            "title": "Multiplexer"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#arithmetic",
            "text": "If we used sum of products form for an adder (adds two binary numbers together), their would be way to many curcuits to reason about with 32/64 inputs. Instead we sequence a smaller curcuit.  Full / 1-bit adder takes in 2 digits and a carry, produces result digit and carry out. We then use multiple copys to create a 32-bit adder (ripple carry adder):",
            "title": "Arithmetic"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#propagation-delay",
            "text": "Propagation delay  Time delay between input signal change and output signal change.   The propagation delay depends on:   Technology (transistor parameters, wire capacitance, etc).  Delay through gate  Number of gates",
            "title": "Propagation delay"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#sequential-logic",
            "text": "Sequential logic  Output depends on current and past input(s), i.e. the curcuit has memory.",
            "title": "Sequential logic"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#sr-latch",
            "text": "The SR latch is the simplest form of memory.   It has the following truth table",
            "title": "SR Latch"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#clock",
            "text": "We know the because of propagation delay, curcuits with lots of gates take some time to output. Outputs may also fluctute if certain paths are shorted than others. The clock forces all state changes to happen when its 1, and guarentees the state is correct while it is 0, thus we avoid any occilation effecting calculations.",
            "title": "Clock"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#level-triggered-d-latch",
            "text": "A latch which only only changes when the clock is 1.   For example   changes before  , but   wont change until after  .",
            "title": "Level-triggered D Latch"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#edge-triggered-d-flip-flop",
            "text": "We can do better. Half a clock cycle is still alot of time, thus an edge-triggered D flip-flop will only change when the input changes on a  positive clock edge .   For example   doesnt cause a change since it happens after the clock edge.",
            "title": "Edge-triggered D flip-flop"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#general-sequential-logic-curcuit",
            "text": "Now if we tie multiple D flip-flops together (with a common clock) we get a register. Combine this with some combinational logic (whos state is saved by the registers to be used next cycle) and we arrive at a general sequential logic circuit.",
            "title": "General sequential logic curcuit"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#hardware-fsm",
            "text": "We can use FSM to derive the seqential logic curcuit.",
            "title": "Hardware FSM"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#processor-design-single-cycle",
            "text": "Datapath  Performs the data operations as controlled by the instructions  Control  Controls the datapath, memory and I/O as controlled by the instructions",
            "title": "Processor Design - Single Cycle"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#main-functions",
            "text": "Fetch  instructions from instruction memory  Read the register operands  Use the ALU for computation  Arithmetic  Memory adress  Branch target    Access data memory for load/store  Store the result of the computation/data into the destination register  Update the program counter (PC)",
            "title": "Main functions"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#r-format-instructions",
            "text": "Read two register operands  Perform the arthemetic/logical operation  Write register result",
            "title": "R-Format instructions"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#loadstore-instructions",
            "text": "Read register operands  Calculare adress using 16-bit offset  Use ALU with sign-extend    Read (for load) or write (for store) the memory.  (Load only) Update destination register",
            "title": "Load/Store instructions"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#branch-instructions",
            "text": "Read register operands  Compare operands  Use ALC, subtact and check Zero output    Calculate target adress  Sign-extend the immediate  Shift left 2 places (word align)  Add to PC + 4",
            "title": "Branch instructions"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#full-datapath",
            "text": "",
            "title": "Full Datapath"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#control-part",
            "text": "The control component produces the signals that control the datapath by looking at the 6 bit op code.",
            "title": "Control part"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#alu-control",
            "text": "ALU operations:     Type  Operation      Data transfers  Add    Branches  Sub    Other  Look at func field     Other operations:   0000  - AND  0001  - OR  0010  - add  0110  - subtract  0111  - set-on-less than",
            "title": "ALU control"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#processor-design-multicycle-processer",
            "text": "Motivations for multicycle processors:   Speed:  In a single cycle processor, the clock cycle must be long enough for the most complex instruction.  Cost:  Functional units (adders e.g.) cannot be re-used during instruction execution.   Basic idea:   Break up execution of instructions to multiple cycles  Ensure actions performed in a cycle are generic (common to many instructions)  Reuse datapath and control path components",
            "title": "Processor Design - Multicycle processer"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#building-blocks",
            "text": "One memory  Shared between instructions and data  Common interface given an indress and data to write, produces the read data.    Registers  Read early in instruction cycle  Written late  No chance of read/write contention    One ALU  All PC calculations  All arithmetic",
            "title": "Building blocks"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#example-execution",
            "text": "<=  means the register \"get the value of\" at the end of the clock cycle.",
            "title": "Example execution"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#fetch",
            "text": "IR <= Mem[PC]1\nPC <= PC + 4   Read registers   A <= Reg[IR[25:21]] // adress the register denoted by the instruction\nB <= Reg[IR[20:16]]\nALUOut <= PC + sgnext(IR[15:0] << 0) // compute branch offset     R-type arithmetic     ALUOut <= A op B   Imediate arithmetic   ALUOut <= A + sgnext(IR[15:0])   Branch completion   if(A == B) PC <= ALUOut   Jump completion   PC <= {PC[31:28], IR[25:0], 2'b00} // 4 most significant bits + jump adress + 00   Perform store",
            "title": "Fetch"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#data-and-control-path",
            "text": "New instruction register to store currently executing instruction  A & B read register  ALU mux to control ALU inputs and operations  ALU out register  Memory data register (stores data from memory)",
            "title": "Data and control path"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#memory",
            "text": "Temporal locality  Recently accessed memeory location is likely to be accessed again in the near future.  Spatial locatility  Memory location close to recently accessed location are likely to be accessed in the near future.",
            "title": "Memory"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#memory-hierachy",
            "text": "Temporal locality   If access data from slower memeory, move to faster memory  If data is faster memeory unused recently, move it to slower memory     Spartial locality   If need to move a word from slower to faster memory, move adjacten words  Gives rise to blocks and pages, units of storage for contiguous memory.",
            "title": "Memory hierachy"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#terminology",
            "text": "Block (or line)  Unit of data stored in the cache  Hit  Data is found in the cache  Miss  Data not found  Hit rate  Fraction of accesses that are hits at any given level of hierarchy  Hit time  Time required to access a level of the hierarchy, including time to determin wherer acess hit or miss  Miss penalty  Extra time required to fetch ablock in the next level down",
            "title": "Terminology"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#cache",
            "text": "Tag  A word indicating the address of the main memroy block it holds (a subsection of the adress)  Valid bit  Indicates the block is in use",
            "title": "Cache"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#fully-associative-cache",
            "text": "The tag is all of the bits an the adress except from the block offset. The byte offset is comprised of the lowest bits that denote the requested byte with in the cache block.",
            "title": "Fully-associative cache"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#cache-replacement",
            "text": "Least recently used (LRU)  Evict the cache block that hasnt been accessed in the longest time  First in first out (FIFO)  Replace in the same order as filled",
            "title": "Cache replacement"
        },
        {
            "location": "/Introduction-to-Computer-Systems/#direct-mapped-cache",
            "text": "A data item can be stored in one location only in the cache. The adress is split into the tag, index and byte offset. The index tells us where the memory is found/stored. If the tag in that position is the same, then their is a hit, otherwise its a miss.",
            "title": "Direct-mapped cache"
        },
        {
            "location": "/Introduction-to-Software-Engineering/",
            "text": ".latex-box.math-false {text-align: center;}\n.math-true {vertical-align: middle;}\n\n\n\nUse Cases\n\n\n\n\nUse case\n\n\nA task involving the system which has value for one or more stakeholders.\n\n\nActors\n\n\nA stakeholder who takes an active part in the use case.\n\n\nPrimary Actor\n\n\nStakeholder with primary intrest in use case (usually the one triggering the use case).\n\n\nSupporting actors\n\n\nActors also envolved\n\n\n\n\nSome stakeholders may not be participating in a use case (so neither primary or supporting actors).\n\n\nActors can be:\n\n\n\n\nUsers of a system\n\n\nAn external system, which interacts with the system\n\n\n\n\nUsually a use case is a sequence of steps, however they may other paths, they may succeed, fail, or succeed in an alternate way.\n\n\nExample\n\n\nMain success scenario\n\n\n1. Customer selects item\n2. Customer checks out\n3. ...\n4. Email confirmation\n\n\n\n\nExtensions - variations of the main success scenario\n\n\n6a. Credit card auth fails\n    .1 Re-enter details\n\n\n\n\nUse case templates\n\n\n\n\nGoal:\n What the primary actor wishes to acheive\n\n\nSummary:\n Description of use case\n\n\nStakeholders and each\u2019s Interest in the use case\n\n\nPrimary actor\n\n\nSupporting actors\n\n\nTrigger:\n The event that leads to this use case being performed.\n\n\nPre-conditions/Assumptions:\n What can be assumed to be true when the use case starts\n\n\nGuarantees:\n What the use case ensures at its end\n\n\nSuccess guarantees\n\n\nFailure guarantees\n\n\nMinimal guarantees\n\n\n\n\n\n\nMain Success Scenario\n\n\nAlternative scenarios\n\n\n\n\nA use case can:\n- Diffrent levels of detail\n    - Depending on part of development process\n- May refer to other use cases\n- Descripe diffrent scopes\n\n\nRequirements capture\n\n\n\n\nIdentify actors\n\n\nFor each actor\n\n\nWhat do they need\n\n\nAny other expected interactions\n\n\nWhich use cases have priority\n\n\n\n\n\n\n\n\nUse cases are often functional requirements, with non-functional requirements attached. Other times. Non-functional requirements apply to subsets or all of use-cases.\n\n\nOther uses\n\n\n\n\nDrive design\n\n\nDesign validation\n\n\nDoes the design work\n\n\n\n\n\n\nTesting\n\n\nGood source for system tests\n\n\n\n\n\n\n\n\nProblems\n\n\n\n\nMay be to much detail (constraining design)\n\n\nSupporting actors may not be strictly necessary i.e. librarian may not be involved in borrowing a book (in a modern library)\n\n\nLess attension to architecture and static object structure.\n\n\nMay miss requirements not associated with actors.\n\n\n\n\nDesign\n\n\n\n\nDesign\n\n\nHow softawre will meet the requirements\n\n\n\n\nOutputs of design:\n\n\n\n\nModels\n\n\nUML / SImulink\n\n\nOften graphical\n\n\nCan be executable\n\n\n\n\n\n\nWritten documents\n\n\nRecord reasons for decisions (\n\n\ntrace back when problem occors to relevent stakeholder\n\n\n\n\n\n\n\n\nCriteria:\n\n\n\n\nThe design can meet the requirements\n\n\nIs it maintainable?\n\n\nCan it be explained to implementors\n\n\nFits constraints of exsisting technology (legacy components)\n\n\nMakes future design choices easy\n\n\n\n\n\n\nHigh level (architectural) design\n\n\nHow subsytems are split up\n\n\nLow level (detailed) design\n\n\nHow subsystems are composed\n\n\n\n\nAt each level:\n\n\n\n\nWhat are the responsibilities of each component?\n\n\nwhat are the interfaces?\n\n\nWhat are the messages exchanged (what protocols)?\n\n\n\n\n\n\nArchitecture\n\n\nThe way that components work together.\n\n\n\n\n\n\n\n\nWhat are the components\n\n\n\n\nWhere shall we put the encapsulation barriers?\n\n\nWhich decisions do we want to hide inside components (so they can be changed without effecting the rest of the system)?\n\n\n\n\n\n\n\n\nWhat are the connectors?\n\n\n\n\nHow/what do the components need to communicate?\n\n\nWhat should be the interfaces?\n\n\nWhat protocols should be used?\n\n\n\n\n\n\n\n\nMore architecutral decisions:\n\n\n\n\nWhat language and/or component standard is going to be used?\n\n\nIs their an appropriate exising framework?\n\n\nWhat conventions for error handling?\n\n\nbackups\n\n\nresiliance\n\n\nfail gracefully\n\n\n\n\n\n\n\n\nDetailed design\n\n\n\n\nArchitecture has be designed\n\n\nEach person/team is in charge of designing one subsystem\n\n\nWhat external interfaces must it work with?\n\n\nWhat classes and behaviour are needed?\n\n\nCoordination with system architect to change interface if required.\n\n\n\n\nPrinciples\n\n\n\n\nCohesion\n\n\nA mesure of the strenth o fthe realationship between the pieces of functionality within the component. High cohesion has increased understandability, maintainability and reliability.\n\n\nCoupling\n\n\nA mesure of the strength of the inter-connection between components. Loose coupling increases understandability and maintainablilty.\n\n\nAbstraction\n\n\nA view of some entity that focuses on the infomation relevent to a particular purpose.\n\n\nEncapsulation\n\n\nGrouping and packaging the elements and internal details of an abstraction and making those details inaccessible.\n\n\nSeperation of interface/implementation\n\n\nSpecifing a public interface, known to the clients, separate from the details of how the component is realized.\n\n\nDecomposition\n\n\nDivinding large systems into smaller components with distinct responsibilitys.\n\n\n\n\nModeling\n\n\n\n\nModel\n\n\nA precise represntation of some of the information needed to solve a problem using a computer.\n\n\n\n\nA UML model\n- represented by set of diagrams\n- structured represnetation too (XML)\n- must obey rules of UML standard\n- (fairly) precise meaning\n- Used informally (talking around whiteboard)\n\n\nBig design up front\n\n\n\n\nOften unavoidable\n\n\nIf done right, simplifys developent\n\n\nError prone (changing requirements)\n\n\nWastefull (mistakes in requirements)\n\n\n\n\nXP maxinms:\n\n\n\n\nYou aint going to need it\n\n\nDo the simplest thing that could possibly work\n\n\n\n\nDynamic aspects of design\n\n\nTheir are two types of behaviour within OO systems\n\n\n\n\nInter-object bhaviour\n\n\nWho sends whcih messages to whom?\n\n\nIntra-object behaviour\n\n\nWhat state changes does each object undergo as it revieves messages? How does this effect its behaviour?\n\n\n\n\nInter-object behaviour\n\n\n\n\nClasses should correspond to domain consepts\n\n\nData encapsulated should match real world model\n\n\nWork out what operations are needed for the usecases\n\n\n\n\nIt may not be obvious when several objects that have to collaborate. CRC cards can help.\n\n\nInteraction diagrams\n\n\nDescribes dynamic interactions, good for showing how the system realized a use case. UML has two diagrams, sequence and communication diagrams.\n\n\n\n\nDecide exactly what behaviour to model\n\n\nName the objects which are involved\n\n\nIdentify the sequence of messages which objects sends\n\n\nRecord this in the syntax of an interaction diagram\n\n\n\n\nGood interaction diagrams\n\n\n\n\nConceptual coherence\n\n\nDoes it make sence for thus class to have that operation\n\n\nMaintainability\n\n\nWhich aspects might change, how hard woull it be to change the itnterataction\n\n\nPerformance\n\n\nIs all the work being done neccessary\n\n\n\n\nReducing long-range coupling\n\n\nConsider three classes \nA\n, \nB\n, \nC\n where \nA -> B -> C\n. If \nA\n needs some data from \nC\n one approach would be for \nB\n to return a reference to \nC\n. However, this means \nA\n and \nC\n are tightly coupled.\n\n\nA better approach is for \nB\n to exactly what \nA\n needs from \nC\n, thus \nA\n is no longer coupled to \nC\n.\n\n\nThe Law of Demeter\n\n\nThe Law of Demeter is a design prenciple ro reduce long range coupling. In responce to a message \nm\n, an object \nO\n should send messages only to the following objects:\n\n\n\n\nO\n itself\n\n\nobjects which are arguments to message \nm\n\n\nobjects which \nO\n creates as part of its reaction to \nm\n\n\nObjects which are directly accessible from \nO\n, that is, using values of attributes of \nO\n\n\n\n\nDesign patterns\n\n\n\n\nDesign pattern\n\n\nA names, well understood good solution to a common problem.\n\n\n\n\nComposite design pattern\n\n\nConsider a graphics library which supports various operations such as move, draw, change color. These operations chould apply to a single object, or a tree of connected objects.\n\n\n\n\nThe composite design pattern introduces another subclass \ngroup\n of the abstract class \ngraphics\n which has a reference to all the graphics in that group. The group itself is a \ngraphics\n, so it can be a subgroup of some other \ngroup\n.\n\n\nThe benifits\n\n\n\n\nAllows trees of any depth\n\n\nNew subclasses dont require any additionaly methods to support the tree structure.\n\n\n\n\nThe drawbacks\n\n\n\n\nMay cause maintance issues (draw methods split along many files)\n\n\nOne solution would be the visitor pattern  \n\n\n\n\n\n\n\n\nObserver pattern\n\n\nThe observer pattern has an object called the \nsubject\n which has a collection of \nobservers\n that it notifys automaticly when any state changes.\n\n\n\n\nTemplate method pattern\n\n\nThe skeleton of an algorithum is defined, defferings certiain steps to subclasses. This allows modification of parts of the algorithum, without changing its original structure. Consider a sorting algothum. For the comparison step, several subclasses can be made such as \nIntCompare\n and \nStringCompare\n so that the algorithum can be used not just with numbers but other types aswell.\n\n\n\n\nElements of a pattern\n\n\n\n\nName\n\n\nAliases (other names for a pattern)\n\n\nContext (where the problem arrises?)\n\n\nProblem (why a naive approach wont work?)\n\n\nSolution (mixture of text and models)\n\n\nConsequences (benifits/drawbacks)\n\n\n\n\nHigh quality code\n\n\n\n\nCodeing standards\n\n\nPlacement of braces\n\n\nIndenting\n\n\n\n\n\n\nUse meaningfull names\n\n\nIf they become out of date, refactor them\n\n\n\n\n\n\nAvoid cryptic comments\n\n\nTry to make code clear instead of comments\n\n\n\n\n\n\nBalance structual complexity vs code duplication\n\n\nDont repeat yourself\n\n\nAvoid costly changes just to avoid writing somthing twise\n\n\n\n\n\n\nBe clever, but no too clever\n\n\nAvoid deprecated, obscure or unstable language features\n\n\nPremature optermization is the root of all evil...\n\n\n\n\n\n\nRemove dead code, unneeded packages, etc\n\n\n\n\nDocumentation\n\n\nAny software projects require documentation for end users. Most languages have doc tools that allow embedded documentation in the code to be transformed into a html page, or other display format.\n\n\nCoding style\n\n\n\n\nDeclarations and local variables\n\n\nLimit scope of local variables\n\n\n\n\n\n\nConditional and loop statements\n\n\nPut the common case first, any exceptions after\n\n\nAvoid deep nesting\n\n\n\n\n\n\nHow code is split\n\n\nAvoid long methods\n\n\n\n\n\n\nDefensive programming\n\n\nUse assertions\n\n\nHandle errors appropriately\n\n\n\n\n\n\nUse suitable OO patterns\n\n\nDivide code into packages\n\n\nAllow related pieces of code to be grouped\n\n\nUnits of encapsulation\n\n\nFields and methods by default are visible to packages\n\n\n\n\n\n\nOrganize namespace\n\n\n\n\n\n\n\n\nOO Features\n\n\n\n\nClasses\n\n\nGrouping behaviour\n\n\nConcetual abstraction\n\n\nHides state\n\n\n\n\n\n\nInheritance\n\n\nDoesnt need to know of private class members\n\n\n\n\n\n\nInterfaces\n\n\nDecouple implementations from users\n\n\n\n\n\n\n\n\nVersion control\n\n\nRefactoring\n\n\nVerification, Validation and Testing\n\n\nJava Modeling language\n\n\nJML is a richer language for writing conditions e.g. quantifiers.\n\n\n\n\nPreconditions: \n//@ requires x > 0;\n\n\nPostconditions: \n//@ ensures \\result % 2 == 0\n\n\nInvariants: \n//@ invariant name.length <= 8\n\n\nGeneral assertions: \n//@ assert i + j = 12;\n\n\n\n\nDynamic analysisi\n\n\nRunning the program with assetions is a basic form of dynamic analysis, their are tools that can compile JML to java to test at runtime\n\n\nStatic analysis\n\n\nSome tests can be run at compile time\n\n\n\n\nChecks code oversights, e.g. did you check if the file exsists\n\n\nCorectness of pre/post condtions of some methods\n\n\nConcurrency bugs e.g race conditions\n\n\n\n\nDeployment and Maintance\n\n\n\n\nBusiness process: Most software requires the customer to change the way they word, has this been thought through?\n\n\nTraining: No point deploying it if the customer can use it.\n\n\nDeployment itself: How the software is physicaly installed\n\n\nEquipment: Is the computer systems compatible\n\n\nExpertices: Is their the personal to marage the deployment\n\n\nIntegration with the businesses other software\n\n\n\n\nDeployment\n\n\n\n\nPackage software\n\n\nMake it availble (cd, usb, internet etc)\n\n\nTurn key installers\n\n\nCheck dependencys\n\n\nInstall software\n\n\nCheck lisences\n\n\n\n\n\n\n\n\nMaintance\n\n\n\n\nFixing bugs and vulnerabilites (including problems in the original design).\n\n\nAdapting to new platforms and software enviroments.\n\n\nSupporting new features and reqirements.\n\n\n\n\nMaintance challenges\n\n\n\n\nOften a new team maintance software\n\n\nDevelopment vs maintance often seperate contracts, developers dont have insentive to make it maintanable\n\n\nUnpopular, less skilled, involve obselete languages\n\n\nStructure degrades\n\n\nCompilers stops working, documentation goes unmaintained\n\n\n\n\nSoftware evolution\n\n\n\n\nGather change requirements\n\n\nNew features\n\n\nAdapting to buissness changes\n\n\nBug reports\n\n\n\n\n\n\nEvaluate, produce list of changes\n\n\nGo through the normal development cycle\n\n\nIssue the new release\n\n\n\n\nIn the case of emergency, this process is often shortcurcuited.\n\n\nRe-engineering\n\n\nRe-engineering is the process of transforming old software until its maintainable\n\n\n\n\nSource code translation\n from obsolete languages.\n\n\nReverse engineering\n analyse the program to derive the code in the case were the source code has been lost.\n\n\nStructure improvement\n, i.e. modularization, architercural refactoring.\n\n\nData re-enginerring\n, reformatting and cleaning up data.\n\n\nAdding adaptor interfaces\n to users and newer software. \n\n\n\n\nBug Reporting\n\n\nMany projects use \nbug tracking systems\n for both bug reports and feature requests. They provide receving, tracking, notifying and monitoring to ensure bugs and features are followed through.\n\n\nWriting a bug report\n\n\n\n\nKeep the report concise\n\n\nInclude tediously detailed infomation\n\n\nHow can you reporoduce it?\n\n\nList everything you did in order.\n\n\n\n\n\n\nInclude full infomation about the computer system\n\n\nMake an attempt to diagnose the problem",
            "title": "Introduction to Software Engineering"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#use-cases",
            "text": "Use case  A task involving the system which has value for one or more stakeholders.  Actors  A stakeholder who takes an active part in the use case.  Primary Actor  Stakeholder with primary intrest in use case (usually the one triggering the use case).  Supporting actors  Actors also envolved   Some stakeholders may not be participating in a use case (so neither primary or supporting actors).  Actors can be:   Users of a system  An external system, which interacts with the system   Usually a use case is a sequence of steps, however they may other paths, they may succeed, fail, or succeed in an alternate way.",
            "title": "Use Cases"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#example",
            "text": "Main success scenario  1. Customer selects item\n2. Customer checks out\n3. ...\n4. Email confirmation  Extensions - variations of the main success scenario  6a. Credit card auth fails\n    .1 Re-enter details",
            "title": "Example"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#use-case-templates",
            "text": "Goal:  What the primary actor wishes to acheive  Summary:  Description of use case  Stakeholders and each\u2019s Interest in the use case  Primary actor  Supporting actors  Trigger:  The event that leads to this use case being performed.  Pre-conditions/Assumptions:  What can be assumed to be true when the use case starts  Guarantees:  What the use case ensures at its end  Success guarantees  Failure guarantees  Minimal guarantees    Main Success Scenario  Alternative scenarios   A use case can:\n- Diffrent levels of detail\n    - Depending on part of development process\n- May refer to other use cases\n- Descripe diffrent scopes",
            "title": "Use case templates"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#requirements-capture",
            "text": "Identify actors  For each actor  What do they need  Any other expected interactions  Which use cases have priority     Use cases are often functional requirements, with non-functional requirements attached. Other times. Non-functional requirements apply to subsets or all of use-cases.",
            "title": "Requirements capture"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#other-uses",
            "text": "Drive design  Design validation  Does the design work    Testing  Good source for system tests",
            "title": "Other uses"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#problems",
            "text": "May be to much detail (constraining design)  Supporting actors may not be strictly necessary i.e. librarian may not be involved in borrowing a book (in a modern library)  Less attension to architecture and static object structure.  May miss requirements not associated with actors.",
            "title": "Problems"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#design",
            "text": "Design  How softawre will meet the requirements   Outputs of design:   Models  UML / SImulink  Often graphical  Can be executable    Written documents  Record reasons for decisions (  trace back when problem occors to relevent stakeholder     Criteria:   The design can meet the requirements  Is it maintainable?  Can it be explained to implementors  Fits constraints of exsisting technology (legacy components)  Makes future design choices easy    High level (architectural) design  How subsytems are split up  Low level (detailed) design  How subsystems are composed   At each level:   What are the responsibilities of each component?  what are the interfaces?  What are the messages exchanged (what protocols)?    Architecture  The way that components work together.     What are the components   Where shall we put the encapsulation barriers?  Which decisions do we want to hide inside components (so they can be changed without effecting the rest of the system)?     What are the connectors?   How/what do the components need to communicate?  What should be the interfaces?  What protocols should be used?",
            "title": "Design"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#more-architecutral-decisions",
            "text": "What language and/or component standard is going to be used?  Is their an appropriate exising framework?  What conventions for error handling?  backups  resiliance  fail gracefully",
            "title": "More architecutral decisions:"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#detailed-design",
            "text": "Architecture has be designed  Each person/team is in charge of designing one subsystem  What external interfaces must it work with?  What classes and behaviour are needed?  Coordination with system architect to change interface if required.",
            "title": "Detailed design"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#principles",
            "text": "Cohesion  A mesure of the strenth o fthe realationship between the pieces of functionality within the component. High cohesion has increased understandability, maintainability and reliability.  Coupling  A mesure of the strength of the inter-connection between components. Loose coupling increases understandability and maintainablilty.  Abstraction  A view of some entity that focuses on the infomation relevent to a particular purpose.  Encapsulation  Grouping and packaging the elements and internal details of an abstraction and making those details inaccessible.  Seperation of interface/implementation  Specifing a public interface, known to the clients, separate from the details of how the component is realized.  Decomposition  Divinding large systems into smaller components with distinct responsibilitys.",
            "title": "Principles"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#modeling",
            "text": "Model  A precise represntation of some of the information needed to solve a problem using a computer.   A UML model\n- represented by set of diagrams\n- structured represnetation too (XML)\n- must obey rules of UML standard\n- (fairly) precise meaning\n- Used informally (talking around whiteboard)",
            "title": "Modeling"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#big-design-up-front",
            "text": "Often unavoidable  If done right, simplifys developent  Error prone (changing requirements)  Wastefull (mistakes in requirements)   XP maxinms:   You aint going to need it  Do the simplest thing that could possibly work",
            "title": "Big design up front"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#dynamic-aspects-of-design",
            "text": "Their are two types of behaviour within OO systems   Inter-object bhaviour  Who sends whcih messages to whom?  Intra-object behaviour  What state changes does each object undergo as it revieves messages? How does this effect its behaviour?",
            "title": "Dynamic aspects of design"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#inter-object-behaviour",
            "text": "Classes should correspond to domain consepts  Data encapsulated should match real world model  Work out what operations are needed for the usecases   It may not be obvious when several objects that have to collaborate. CRC cards can help.",
            "title": "Inter-object behaviour"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#interaction-diagrams",
            "text": "Describes dynamic interactions, good for showing how the system realized a use case. UML has two diagrams, sequence and communication diagrams.   Decide exactly what behaviour to model  Name the objects which are involved  Identify the sequence of messages which objects sends  Record this in the syntax of an interaction diagram",
            "title": "Interaction diagrams"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#good-interaction-diagrams",
            "text": "Conceptual coherence  Does it make sence for thus class to have that operation  Maintainability  Which aspects might change, how hard woull it be to change the itnterataction  Performance  Is all the work being done neccessary",
            "title": "Good interaction diagrams"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#reducing-long-range-coupling",
            "text": "Consider three classes  A ,  B ,  C  where  A -> B -> C . If  A  needs some data from  C  one approach would be for  B  to return a reference to  C . However, this means  A  and  C  are tightly coupled.  A better approach is for  B  to exactly what  A  needs from  C , thus  A  is no longer coupled to  C .",
            "title": "Reducing long-range coupling"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#the-law-of-demeter",
            "text": "The Law of Demeter is a design prenciple ro reduce long range coupling. In responce to a message  m , an object  O  should send messages only to the following objects:   O  itself  objects which are arguments to message  m  objects which  O  creates as part of its reaction to  m  Objects which are directly accessible from  O , that is, using values of attributes of  O",
            "title": "The Law of Demeter"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#design-patterns",
            "text": "Design pattern  A names, well understood good solution to a common problem.",
            "title": "Design patterns"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#composite-design-pattern",
            "text": "Consider a graphics library which supports various operations such as move, draw, change color. These operations chould apply to a single object, or a tree of connected objects.   The composite design pattern introduces another subclass  group  of the abstract class  graphics  which has a reference to all the graphics in that group. The group itself is a  graphics , so it can be a subgroup of some other  group .  The benifits   Allows trees of any depth  New subclasses dont require any additionaly methods to support the tree structure.   The drawbacks   May cause maintance issues (draw methods split along many files)  One solution would be the visitor pattern",
            "title": "Composite design pattern"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#observer-pattern",
            "text": "The observer pattern has an object called the  subject  which has a collection of  observers  that it notifys automaticly when any state changes.",
            "title": "Observer pattern"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#template-method-pattern",
            "text": "The skeleton of an algorithum is defined, defferings certiain steps to subclasses. This allows modification of parts of the algorithum, without changing its original structure. Consider a sorting algothum. For the comparison step, several subclasses can be made such as  IntCompare  and  StringCompare  so that the algorithum can be used not just with numbers but other types aswell.",
            "title": "Template method pattern"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#elements-of-a-pattern",
            "text": "Name  Aliases (other names for a pattern)  Context (where the problem arrises?)  Problem (why a naive approach wont work?)  Solution (mixture of text and models)  Consequences (benifits/drawbacks)",
            "title": "Elements of a pattern"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#high-quality-code",
            "text": "Codeing standards  Placement of braces  Indenting    Use meaningfull names  If they become out of date, refactor them    Avoid cryptic comments  Try to make code clear instead of comments    Balance structual complexity vs code duplication  Dont repeat yourself  Avoid costly changes just to avoid writing somthing twise    Be clever, but no too clever  Avoid deprecated, obscure or unstable language features  Premature optermization is the root of all evil...    Remove dead code, unneeded packages, etc",
            "title": "High quality code"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#documentation",
            "text": "Any software projects require documentation for end users. Most languages have doc tools that allow embedded documentation in the code to be transformed into a html page, or other display format.",
            "title": "Documentation"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#coding-style",
            "text": "Declarations and local variables  Limit scope of local variables    Conditional and loop statements  Put the common case first, any exceptions after  Avoid deep nesting    How code is split  Avoid long methods    Defensive programming  Use assertions  Handle errors appropriately    Use suitable OO patterns  Divide code into packages  Allow related pieces of code to be grouped  Units of encapsulation  Fields and methods by default are visible to packages    Organize namespace",
            "title": "Coding style"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#oo-features",
            "text": "Classes  Grouping behaviour  Concetual abstraction  Hides state    Inheritance  Doesnt need to know of private class members    Interfaces  Decouple implementations from users",
            "title": "OO Features"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#version-control",
            "text": "",
            "title": "Version control"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#refactoring",
            "text": "",
            "title": "Refactoring"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#verification-validation-and-testing",
            "text": "",
            "title": "Verification, Validation and Testing"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#java-modeling-language",
            "text": "JML is a richer language for writing conditions e.g. quantifiers.   Preconditions:  //@ requires x > 0;  Postconditions:  //@ ensures \\result % 2 == 0  Invariants:  //@ invariant name.length <= 8  General assertions:  //@ assert i + j = 12;",
            "title": "Java Modeling language"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#dynamic-analysisi",
            "text": "Running the program with assetions is a basic form of dynamic analysis, their are tools that can compile JML to java to test at runtime",
            "title": "Dynamic analysisi"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#static-analysis",
            "text": "Some tests can be run at compile time   Checks code oversights, e.g. did you check if the file exsists  Corectness of pre/post condtions of some methods  Concurrency bugs e.g race conditions",
            "title": "Static analysis"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#deployment-and-maintance",
            "text": "Business process: Most software requires the customer to change the way they word, has this been thought through?  Training: No point deploying it if the customer can use it.  Deployment itself: How the software is physicaly installed  Equipment: Is the computer systems compatible  Expertices: Is their the personal to marage the deployment  Integration with the businesses other software",
            "title": "Deployment and Maintance"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#deployment",
            "text": "Package software  Make it availble (cd, usb, internet etc)  Turn key installers  Check dependencys  Install software  Check lisences",
            "title": "Deployment"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#maintance",
            "text": "Fixing bugs and vulnerabilites (including problems in the original design).  Adapting to new platforms and software enviroments.  Supporting new features and reqirements.",
            "title": "Maintance"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#maintance-challenges",
            "text": "Often a new team maintance software  Development vs maintance often seperate contracts, developers dont have insentive to make it maintanable  Unpopular, less skilled, involve obselete languages  Structure degrades  Compilers stops working, documentation goes unmaintained",
            "title": "Maintance challenges"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#software-evolution",
            "text": "Gather change requirements  New features  Adapting to buissness changes  Bug reports    Evaluate, produce list of changes  Go through the normal development cycle  Issue the new release   In the case of emergency, this process is often shortcurcuited.",
            "title": "Software evolution"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#re-engineering",
            "text": "Re-engineering is the process of transforming old software until its maintainable   Source code translation  from obsolete languages.  Reverse engineering  analyse the program to derive the code in the case were the source code has been lost.  Structure improvement , i.e. modularization, architercural refactoring.  Data re-enginerring , reformatting and cleaning up data.  Adding adaptor interfaces  to users and newer software.",
            "title": "Re-engineering"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#bug-reporting",
            "text": "Many projects use  bug tracking systems  for both bug reports and feature requests. They provide receving, tracking, notifying and monitoring to ensure bugs and features are followed through.",
            "title": "Bug Reporting"
        },
        {
            "location": "/Introduction-to-Software-Engineering/#writing-a-bug-report",
            "text": "Keep the report concise  Include tediously detailed infomation  How can you reporoduce it?  List everything you did in order.    Include full infomation about the computer system  Make an attempt to diagnose the problem",
            "title": "Writing a bug report"
        }
    ]
}